{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dud2cKI_jQPY"
      },
      "source": [
        "# Intelligent Systems in Medical Imaging 2023\n",
        "\n",
        "This Jupyter notebook is part of the course Intelligent Systems in Medical Imaging (ISMI) from Radboud University (Nijmegen, Netherlands), and it was developed by researchers of Radboud University Medical Center (Nijmegen, Netherlands).\n",
        "\n",
        "You should have obtained this notebook by downloading it from the official Brightspace page of the course.\n",
        "\n",
        "This notebook formulates an assignment as part of the ISMI course, and the content of this notebook should be used solely to develop a solution to this assignment. You should not make the code provided in this notebook, or your own solution, publicly available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a65SNjcojQPk"
      },
      "source": [
        "## Teaching Assistants\n",
        "\n",
        "* Bram de Wilde (bram.dewilde@radboudumc.nl)\n",
        "* Pierpaolo Vendittelli (pierpaolo.vendittelli@radboudumc.nl)\n",
        "* Joeran Bosma (joeran.bosma@radboudumc.nl)\n",
        "* Stephan Dooper (stephan.dooper@radboudumc.nl)\n",
        "\n",
        "For questions about the assignments that go beyond the content, you can contact Bram de Wilde. Questions about the content are addressed in the tutorial sessions. You are also encouraged to use the Brightspace discussion forums to discuss content of the assignments. We will also keep an eye out there to help!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTgsrZe2jQPm"
      },
      "source": [
        "## Guidelines and instructions\n",
        "Make sure you write code in any place that says \"YOUR CODE HERE\" by substituting `None` variables or by adding your own solution. Make sure you write in any place that says \"YOUR ANSWER HERE\" with your answers to the questions.\n",
        "\n",
        "Please **do not delete or add any cells**.\n",
        "\n",
        "Before you turn this problem in, make sure everything runs without errors. The easiest way to check this is to restart the kernel and run all cells (in the menubar, select Runtime$\\rightarrow$Restart & Run All).\n",
        "\n",
        "* Groups: You should work in **groups of 2 or 3 people**. (groups of 2 are preferred!)\n",
        "* You are expected to work in Google Colab. If you run the notebooks locally, you may have to solve some issues yourself!\n",
        "* Submit your **fully executed** notebook to Brightspace with file name format: `GroupN_NameSurname1_NameSurname2_NameSurname3.ipynb`\n",
        "* The deadlines for all assignments are on Brightspace.\n",
        "* Deadlines are soft, but make an effort to be on time. We prioritise feedback on assignments that are handed in before the deadline.\n",
        "* Each assignment has 100 points, your grade is your total number of points divided by 10.\n",
        "* The assignments are mandatory, but **do not count** towards your final grade for the course.\n",
        "* For assignments where you have to submit to grand-challenge.org, use team name format `ismi-GroupN-nickname1`.\n",
        "* When working with Google Colab, we advise you to download model checkpoints (.h5 files). This way you don't lose your checkpoint if your session times out. Also, don't forget to connect to a **GPU runtime** when training neural networks!\n",
        "* In Google Colab, you can mount your Google Drive to save files, by clicking the Folder icon on the left, and then click the Mount Drive icon.\n",
        "\n",
        "There are more detailed instructions on Brightspace on how to use Google Colab for the assignments. You can find it here: https://brightspace.ru.nl/d2l/le/content/333312/Home"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuaO-PnSjQPo"
      },
      "source": [
        "## Students\n",
        "Please fill in this cell with your names, e-mail address and s-numbers. This information will be used to grade your assignment.\n",
        "\n",
        "* [Ivan Slootweg], [s1001424], [ilse.slootweg@ru.nl]\n",
        "* [Elina Antonova], [s1057069], [elina.antonova@ru.nl]\n",
        "* [Sven van der Post], [s1028679], [sven.vanderpost@ru.nl]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4620572b2ac8d472c5688bc1ea84bb0d",
          "grade": false,
          "grade_id": "cell-861c27eb27194612",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "jN-AhZlvjQPq"
      },
      "source": [
        "# Assignment 3 - Training a Neural Network for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "675704773ec41dbeea959e8ae7e938dd",
          "grade": false,
          "grade_id": "cell-c241eae4e0ff3f78",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "HTD4ZNDBjQPr"
      },
      "source": [
        "## Introduction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4ff19d078c81b555811ec73c0de1c64d",
          "grade": false,
          "grade_id": "cell-0a64e3ba4a69fc28",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "mkD4v63ijQPs"
      },
      "source": [
        "\n",
        "\n",
        "A pulmonary nodule is a small round or oval-shaped growth in the lung. It may also be called a “spot on the lung” or a “coin lesion.” Pulmonary *nodules* are smaller than three centimeters (around 1.2 inches) in diameter. If the growth is larger than that, it is called a pulmonary *mass* and is more likely to represent a cancer than a [nodule](http://my.clevelandclinic.org/health/articles/pulmonary-nodules).\n",
        "\n",
        "Nodules can be detected in chest CT images as objects with some kind of rounded shape (even though it is not always the case), which have an intensity that is higher (brighter) than the parenchyma tissue in the lungs.\n",
        "\n",
        "If a nodule is detected, guidelines have to be followed to decide what is the best management for the patient.\n",
        "For this purpose, the [LungRADS guidelines](https://www.acr.org/quality-safety/resources/lungrads) have been released, which describe the type of follow-up analysis based on the type and size of detected nodules.\n",
        "The main categories of nodules considered in LungRADS are 5:\n",
        "* solid nodule\n",
        "* ground-glass nodules (also called GGN, non-solid nodules)\n",
        "* semi-solid nodules (also called part-solid nodules)\n",
        "* calcified nodules\n",
        "* spiculated nodules\n",
        "\n",
        "\n",
        "\n",
        "**Solid** nodules are characterized by a homogeneous texture, a well-defined shape and an intensity above -450\n",
        "Housfield Units (HU) on CT. **Spiculated** nodules appear as solid lesions with characteristic spikes at the border, often considered as an indicator of malignancy. **Non-Solid** nodules (also called ground-glass opacities) have an intensity on CT lower than solid nodules (above -750 HU). **Part-Solid** nodules (also called semi-solid nodules) contain both a non-solid and a solid part, the latter normally referred to as the solid core. Compared with solid nodules, non-solid and part-solid nodules have a higher frequency of being malignant lesions. Finally, **calcified** nodules are characterized by a high intensity and a well-defined rounded shape on CT. If a nodule is completely calcified, it is a benign lesion.\n",
        "\n",
        "The first figure below shows the table used in LungRADS, which you can also find in ```./literature/AssessmentCategories.pdf```.\n",
        "As you can see, the five categories are mentioned in the table, as well as nodule size.\n",
        "While nodule size is something that can be easily measured using a segmentation software, the discrimination of nodule types is not trivial. The second figure below shows examples of pulmonary nodules at different scales. For each nodule, a 2D view in the axial, coronal and sagittal view is shown.\n",
        "\n",
        "<img src=\"https://github.com/ivanslootweg/ISMI/blob/main/assignment_3/figures/lungrads.png?raw=1\" alt=\"LungRADS guidelines\" align=\"middle\" width=\"800\">\n",
        "\n",
        "<img src=\"https://github.com/ivanslootweg/ISMI/blob/main/assignment_3/figures/nodules.png?raw=1\" alt=\"Nodules\" align=\"middle\" width=\"800\">\n",
        "\n",
        "In this assignment, we are going to develop a system based on machine learning to automatically classify pulmonary nodules detected in chest CT scans. For this purpose, we will use data from the publicly available [LIDC-IDRI dataset](https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI). In LIDC-IDRI, nodules have been annotated and labeled by four radiologists. Based on their annotations, we extracted a subset of nodules that will be used in this assignment for training and test purposes.\n",
        "\n",
        "The idea of this assignment is to develop a **multi-class classification system** using machine learning, in particular using **neural networks**. The goal is to achieve the best classification accuracy on the test set, which contains 50 nodules for each class. For each nodule in both the training and test set, we provide both raw data (cubes of 40x40x40 mm containing nodules) and a pre-computed learned representation of nodules, specifically a feature vector of 256 values (more details are provided later).\n",
        "The purpose of this assignment is three-fold:\n",
        "\n",
        "1. Use the features provided to develop a system based on neural networks to classify pulmonary nodule type\n",
        "2. Modify the architecture and the hyper-parameters of the neural networks and investigate how performance changes\n",
        "3. Design and extract new features from raw data, and use them in your classification framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d6702f4155a3851d64b76d022e7b648c",
          "grade": false,
          "grade_id": "cell-459cf760e8e94d6c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "FU2j13tQjQPw"
      },
      "source": [
        "## Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0c2b3403fac04994ae1e7867a1b96b48",
          "grade": false,
          "grade_id": "cell-174c994ececd210f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "sJSASxJCjQPx"
      },
      "source": [
        "We will use data from the publicly available [LIDC-IDRI dataset](https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI).\n",
        "From this dataset, we selected nodules to use in this assignment. In particular, we made a training set and a test set.\n",
        "In both sets, five nodule types are included, namely:\n",
        "* Solid\n",
        "* Non-solid (ground-glass)\n",
        "* Part-solid (semi-solid)\n",
        "* Calcified\n",
        "* Solid spiculated\n",
        "\n",
        "**Training set** The training set *contains a distribution of nodules types that resembles what is typically found in radiology: most of the nodules are solid*. As a consequence, you will notice that the distributions of classes are skewed.\n",
        "\n",
        "**Validation set** We don't provide a validation set. As part of the assignment, you will be asked to build a validation set yourself.\n",
        "\n",
        "**Test set** The test set contains a *balanced* distribution of nodules, meaning that we randomly selected the same amount of nodules per class.\n",
        "\n",
        "### Data details\n",
        "We provide training and test data in two formats: (a) a **dataset** format and (b) a **raw data** format.\n",
        "\n",
        "#### Dataset format\n",
        "The first format is a typical **dataset** format, a matrix, where each row is a sample and each column is a feature. In this\n",
        "dataset, we provide 256 features per nodule. At this stage you don't need to care too much about the source of these features (we will tell you more about that later), just consider that these features are somehow descriptive of pulmonary nodules.\n",
        "The files in dataset format are ```training_set.npz``` and ```test_set.npz```.\n",
        "In the first part of this assignment, you will be asked to train and validate neural networks using the features provided.\n",
        "The training dataset contains data in the field ```['x']``` and labels in the field ```['y']```, while the test set only contains data in the field ```['x']```. The test set also contains a fiels ```['nodule_ids']```, which has unique nodule identifiers that will be used to build the file to submit to [grand-challenge.org](https://grand-challenge.org).\n",
        "The nodule ID has the following format:\n",
        "\n",
        "* ```seriesuid_worldx_worldy_worldz_diameter```\n",
        "\n",
        "where ```seriesuid``` is the code that identifies the scan, ```worldx```, ```y``` and ```z``` indicate the position of the nodule in the scan in world coordinates, and ```diameter``` indicates the diameter of the nodule in *mm*. The position is not really important in this assignment, since we extracted the nodules from the scans for you, but in case you want to trace back these nodules, you know where you can find them in the LIDC-IDRI scans. The diameter may be useful in the last (optional) part of this assignment.\n",
        "\n",
        "#### Raw data format\n",
        "We also provide raw nodule data, because in the optional part of this assignment, you will be asked to train and validate neural networks using raw nodule data. In this part of the assignment, raw data will be available, meaning that you will have the chance of processing data as you like, directly feeding data to the neural network, or extracting additional features, etc.\n",
        "Since pulmonary nodules are extracted from CT scans, raw data is 3D data.\n",
        "Therefore, what we provide is a *cube* for each nodules with size 64x64x64 px, which corresponds to 40x40x40*mm* (we resampled the scans before nodule extraction).\n",
        "\n",
        "In the training set, raw nodule data is organized in folders grouped per nodule type. We think that this may be convenient for you. Cubes are stored as ```npz``` files, and HU intensities are stored in the field ```['data']```, while the class label is stored in the field ```['label']``` for each nodule. Each file is named using the ```nodule_id```, where we also append the actual nodule label at the end of the file name.\n",
        "\n",
        "In the test set, you will find a similar structure as for the training set, but labels are not provided and nodules are not organized in folders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "94172f812b5b45d702a496ca616efad0",
          "grade": false,
          "grade_id": "cell-24128225c117582e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "7JTlIZ7wjQPy"
      },
      "source": [
        "# Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "64a2c400fda3f3d8a5e3b136f1c4e31c",
          "grade": false,
          "grade_id": "cell-rstiraesnxcxccx",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "W-6kIMWIjQPz"
      },
      "source": [
        "The tasks for this assignment are the following.\n",
        "\n",
        "\n",
        "### 1. Train a basic neural network with given nodule-related features (70 points)\n",
        "We have computed features of nodules that you can use for this first part of the assignment. The feature vector contains 256 features per sample, you can find them in ```LIDC-IDRI/training/training_set.npz``` and ```LIDC-IDRI/test/test_set.npz```.\n",
        "* Report results using the shallow neural network architecture proposed in this first part of the assignment.\n",
        "\n",
        "### 2. Improve the neural network by adding more layers (20 points)\n",
        "For the second part of the assignment you need to improve the performance of your network. You can do this, for example, by adding extra layers, or by fine-tuning the hyper-parameters.\n",
        "* Report results using neural networks, experiment with several architectures, hyper-parameters, learning rates, etc.\n",
        "\n",
        "### 3. Train a neural network with raw nodule data (max 10 points)\n",
        "For the third and optional part of the assignment you will have to deal with raw data, you can find them in the directory ```nodules``` for training and test respectively.\n",
        "* Report results using neural networks, experiment with several architectures, hyper-parameters\n",
        "* Design and extract your own features from the raw data provided"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9c22b0d52f963a9542d1f853a5ffcf01",
          "grade": false,
          "grade_id": "cell-806211ea7fdf905e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "Lm0YjyxCjQP0"
      },
      "source": [
        "# Task 1: Train a neural network with given nodule-related features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O39VTeg3jQP0"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q --upgrade ismi_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f824e1680c7cb54ca287d6809473ecdd",
          "grade": false,
          "grade_id": "cell-e8d1eaee073b3ac5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "OxepCnu9jQP1"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import sklearn\n",
        "import sklearn.neighbors\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from IPython import display\n",
        "from tqdm.notebook import tqdm\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from ismi_utils import download_data\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7d2a12f1261ded5ddf447dfa5640829c",
          "grade": false,
          "grade_id": "cell-ab3924be12c44328",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "38-iknJpjQP2"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "565f5b585e9d994119b12b58ae6dc538",
          "grade": false,
          "grade_id": "cell-9eb9d0733831c772",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "IiuVlOpqjQP2"
      },
      "outputs": [],
      "source": [
        "file_name = Path(\"LIDC-IDRI.zip\")\n",
        "download_data(file_name, link='https://surfdrive.surf.nl/files/index.php/s/IglByZUcav5BTtv/download')\n",
        "data_dir = Path(\"LIDC-IDRI\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "da6561d64f810fd6c9af05a4bc3e1b6e",
          "grade": false,
          "grade_id": "cell-352bf4ac36bfde4c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "bEXKAbctjQP3"
      },
      "source": [
        "Set the filepaths to local folders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "36634654ad7b4d027dfb3bb4f146a6dd",
          "grade": false,
          "grade_id": "cell-0619735e80a964d2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "R2PDLe17jQP3"
      },
      "outputs": [],
      "source": [
        "network_dir = Path('.')          # path to store neural network model/weights data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4d326632ddbc5cbb1f7708798df32d1c",
          "grade": false,
          "grade_id": "cell-6e39ceb99648b073",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "IJBW4AMvjQP3"
      },
      "outputs": [],
      "source": [
        "# labels for the five classes in this classification problem.\n",
        "# These are the same names used in folders for raw training data\n",
        "noduleTypes = [\"Solid\", \"Calcified\", \"PartSolid\", \"GroundGlassOpacity\", \"SolidSpiculated\"]\n",
        "n_classes = len(noduleTypes)# len returns the number of element in a list\n",
        "print(f'Classification problem with *{n_classes}* classes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "39572a3e456636770c987ad60acc983a",
          "grade": false,
          "grade_id": "cell-1fc263cdda6be826",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "axsoPHEwjQP4"
      },
      "outputs": [],
      "source": [
        "# convenience function\n",
        "def get_file_list(path,ext='',queue=''):\n",
        "    if ext != '': return [os.path.join(path,f) for f in os.listdir(path) if f.endswith(''+queue+'.'+ext+'')],  [f for f in os.listdir(path) if f.endswith(''+queue+'.'+ext+'')]    \n",
        "    else: return [os.path.join(path,f) for f in os.listdir(path)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "740d6cfc2787f9545fe184770561d340",
          "grade": false,
          "grade_id": "cell-0d1b79d1d3d1dcce",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "PiU08t-6jQP4"
      },
      "source": [
        "### Get to know your data!\n",
        "The data you have now is stored in cubes per nodule, but originally belongs <img src=\"https://github.com/ivanslootweg/ISMI/blob/main/assignment_3/figures/orthogonalviews.jpg?raw=1\" align='right'> to a Chest CT scan. We have three orthogonal orientations: axial, coronal and sagittal orientations. Each cube is $64\\times64\\times64$ px (40x40x40 *mm*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a98ae91cf0db3f8a64f6c5cf3b0edc42",
          "grade": false,
          "grade_id": "cell-30197ae2098a7caf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "OkDvjuDWjQP4"
      },
      "source": [
        "Below is a function that, given a cube containing nodule raw 3D data, returns three orthogonal 2D patches, corresponding to the *axial*, *coronal* and *sagittal* views. Use the figure on the right in this notebook as reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dfbb5501ccfc502f5bb305bc8b0004fa",
          "grade": false,
          "grade_id": "cell-0d3d32ea5755934e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_Mz2XoWcjQP4"
      },
      "outputs": [],
      "source": [
        "# convenience function that returns the axial, coronal and sagittal view given a cube (in numpy format) containing a nodule\n",
        "def get_orthogonal_patches(x):\n",
        "    dims = x.shape # Get shape of X\n",
        "    axial = x[dims[0]//2,:,:]. squeeze()\n",
        "    coronal = x[:,:,dims[2]//2].squeeze()\n",
        "    sagittal= x[:,dims[1]//2,:].squeeze()\n",
        "    return axial, coronal, sagittal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a154852a685bdfff3a466c508ae5ad84",
          "grade": false,
          "grade_id": "cell-df65212a23982091",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "uM9VEzW3jQP5"
      },
      "source": [
        "How does the array indexing (`x[...dims[n] ...]`) in the convenience function `get_orthogonal_patches()` relate to the corresponding axial, coronal and sagittal views?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a85c004502cfb6307191572b3bf3ea06",
          "grade": true,
          "grade_id": "array-indexing-get-orthogonal-patches",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "JmSvAo5ijQP5"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "372e2a400b030e3578df079247c4422a",
          "grade": false,
          "grade_id": "cell-4a688ed703d48723",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "nfCmxPaajQP5"
      },
      "source": [
        "Now visualize some nodules in the training set in axial, coronal and sagittal orientations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5c5984779b0ef04e82028c1d19e54f7d",
          "grade": false,
          "grade_id": "cell-f9df25bfc8ca9105",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "sLiI6EpNjQP6"
      },
      "outputs": [],
      "source": [
        "src_dir = data_dir / \"training\" / \"nodules\"\n",
        "for noduleType in (noduleTypes):\n",
        "    nodules_dir = src_dir / noduleType\n",
        "    npzs = get_file_list(nodules_dir, 'npz')\n",
        "    \n",
        "    for idx1, f in enumerate(range(len(npzs[0]))):\n",
        "        if idx1 > 5:\n",
        "            break\n",
        "        file_path = npzs[0][f]\n",
        "        filename = npzs[1][f]\n",
        "        # axes are oriented as (z, x, y)\n",
        "        npz = np.load(file_path)\n",
        "        axial, coronal, sagittal = get_orthogonal_patches(npz['data'])\n",
        "        plt.suptitle(noduleType)\n",
        "        plt.subplot(1,3,1)\n",
        "        plt.imshow(axial   , cmap='gray', vmin=-600-800, vmax=-600+800); plt.title('axial')\n",
        "        plt.subplot(1,3,2)\n",
        "        plt.imshow(coronal , cmap='gray', vmin=-600-800, vmax=-600+800); plt.title('coronal')\n",
        "        plt.subplot(1,3,3)\n",
        "        plt.imshow(sagittal, cmap='gray', vmin=-600-800, vmax=-600+800); plt.title('sagittal')\n",
        "        plt.show()\n",
        "        \n",
        "shape_cube = npz['data'].shape\n",
        "print ('Size of our cubes',shape_cube)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f9799df6ccb406cd4d9cb67f9de2e233",
          "grade": false,
          "grade_id": "cell-68bdb8ae890a9dae",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "yHS6_U5hjQP6"
      },
      "source": [
        "The quality of patches looks better in the axial view than in the coronal and sagittal view. Bearing in mind that re-sampling took place in the x and y directions, why do you think we get this difference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3d8b2d8517d35eaaff77a33fe955b5f1",
          "grade": true,
          "grade_id": "why-better-axial",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "toKyXm8zjQP6"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "052b91c9511651a7b2f02dd846a2de98",
          "grade": false,
          "grade_id": "cell-ea457d18c102c023",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "EigDwvfxjQP6"
      },
      "source": [
        "### Load datasets\n",
        "Here we are going to load the pre-computed feature vectors of 256 features per sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b064ed315f3ec236eea3023022bc45dc",
          "grade": false,
          "grade_id": "cell-d5f21b5b64b62a1b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3Z1eqBxpjQP6"
      },
      "outputs": [],
      "source": [
        "# load training data (given features)\n",
        "npz = np.load(data_dir / 'training' / 'training_set.npz')\n",
        "x = npz['x']\n",
        "y = npz['y']\n",
        "print (x.shape)\n",
        "print (y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fee5ee0ee4ef39556f40e72d20de4851",
          "grade": false,
          "grade_id": "cell-b4696d27e1c61a2d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "-zMNZ9ORjQP7"
      },
      "source": [
        "Please explain the difference in shape between the x and y arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "745ebfe87956f6140787206164434ecb",
          "grade": true,
          "grade_id": "diff-dims-x-y-arrays",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "krsynTxxjQP7"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b9e431b4e1b38af9d966da18850a729e",
          "grade": false,
          "grade_id": "cell-a7876ab912487eab",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "qzH5dBaBjQP7"
      },
      "outputs": [],
      "source": [
        "# load test data (given features)\n",
        "npz = np.load(data_dir / 'test' / 'test_set.npz')\n",
        "x_test = npz['x']\n",
        "nodule_ids_test = npz['nodule_ids']\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5ee266a953fd6559f14290fea3a7b64a",
          "grade": false,
          "grade_id": "cell-16e75412966e6eb1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "xbvd9y6sjQP7"
      },
      "source": [
        "Please explain the output of `print(x_test.shape)`, which prints the shape of the x (input) test data. What does the first value in () represent? The second value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cf6f6abc04b82ad6e415098804987dbd",
          "grade": true,
          "grade_id": "test-data-shape",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "oT_WLZWXjQP7"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e4d3de3fbd615a0c5b997b1f149e14f3",
          "grade": false,
          "grade_id": "cell-cf2f8929588c3cf9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "o7uT6hHJjQP8"
      },
      "source": [
        "### Training and validation sets\n",
        "\n",
        "The dataset we are given does not explicitly provide a validation set. Therefore, we will have to split the dataset into training and validation sets. Most modern frameworks support splitting datasets and you generally will not need to implement such functionality yourself. For instance, the [Splitter Classes](https://scikit-learn.org/stable/modules/classes.html#splitter-classes) in `scikit-learn` are excellent. However, for educational purposes, it is useful to look at how such a function may be implemented which allows for correcting for class imbalance in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b5bc9db29f6b6e714076f873b5babbfb",
          "grade": false,
          "grade_id": "cell-7dfc97278c3db6fc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "sTMs6MxXjQP8"
      },
      "outputs": [],
      "source": [
        "# one possible implementation of how to split a training set into training and validation subsets\n",
        "def split_training_validation_datasets(x, y, n_classes, val_percentage=0.3, val_balanced=True):\n",
        "    \"\"\"\n",
        "    Derive a training and a validation dataset from a given dataset with\n",
        "    data (x) and labels (y). By default, the validation set is 30% of the\n",
        "    training set, and it has balanced samples across classes. When balancing,\n",
        "    it takes the 30% of the class with the least samples as reference.\n",
        "    \"\"\"\n",
        "    # define number of samples\n",
        "    n_samples = x.shape[0]\n",
        "    \n",
        "    # make array of indexes of all samples [0, ..., n_samples -1]\n",
        "    idxs = np.array(range(n_samples))\n",
        "    \n",
        "    # initialize (empty) lists of samples that will be part of training and validation sets \n",
        "    tra_idxs = []\n",
        "    val_idxs = []\n",
        "    \n",
        "    # append values to tra_idxs and val_idxs by adding the index of training and validation samples\n",
        "    # take into account the input parameters 'val_percentage' and 'val_balanced'\n",
        "    \n",
        "    # find min samples per class\n",
        "    if val_balanced:\n",
        "        min_samples = np.inf\n",
        "        for c in range(n_classes):\n",
        "            n_samples_class = np.sum(y==c)\n",
        "            min_samples = min(min_samples, n_samples_class)\n",
        "    \n",
        "    # split dataset\n",
        "    for c in range(n_classes):\n",
        "        c_idxs = idxs[y==c]\n",
        "        n_idxs = len(c_idxs)\n",
        "        if val_balanced:\n",
        "            val_idxs += list(c_idxs[:int(val_percentage*min_samples)])\n",
        "            tra_idxs += list(c_idxs[int(val_percentage*min_samples):])\n",
        "        else:\n",
        "            val_idxs += list(c_idxs[:int(val_percentage*n_idxs)])\n",
        "            tra_idxs += list(c_idxs[int(val_percentage*n_idxs):])\n",
        "            \n",
        "    print('validation samples = {}'.format(len(val_idxs)))\n",
        "    print('training samples   = {}'.format(len(tra_idxs)))\n",
        "    x_train = x[tra_idxs]\n",
        "    y_train = y[tra_idxs]\n",
        "    x_validation = x[val_idxs]\n",
        "    y_validation = y[val_idxs]\n",
        "    return x_train, y_train, x_validation, y_validation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e0337324c24c77e3f7e4b74b9b2c302d",
          "grade": false,
          "grade_id": "cell-2707071a24f1f86c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "r0uVX21ajQP8"
      },
      "source": [
        "What is class imbalance and why is it an issue for medical imaging data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1b2e1cdf30010508d732e2c038f053aa",
          "grade": true,
          "grade_id": "class-imbalance",
          "locked": false,
          "points": 10,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "MGLrb8QXjQP8"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cceb7fdf7819e08cb4e12358bcaf4164",
          "grade": false,
          "grade_id": "cell-2d7543805686a5eb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "vV4N_uy4jQP9"
      },
      "outputs": [],
      "source": [
        "# split dataset\n",
        "x_train, y_train, x_validation, y_validation = split_training_validation_datasets(x, y, n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "50374d6c813a0c4979f21600c5f85fc8",
          "grade": false,
          "grade_id": "cell-877505d7b1b756a8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "49UihjbZjQP9"
      },
      "source": [
        "As in assignment 2, we normalize the training and test data to have zero mean and unit standard deviation (z-normalization). You should calculate the mean and standard deviation from the training dataset and use these values to z-normalize both the training and test datasets.\n",
        "\n",
        "Normalize:\n",
        "1. The `x_train` training data, storing the result in a new array `x_train_norm`.\n",
        "2. The `x_validation` and `x_test` data, storing the result in new arrays `x_validation_norm` and `x_test_norm`.\n",
        "\n",
        "First of all, normalize the `x_train` data, remembering to keep the values of `x_mean` and `x_std` in order to normalize the validation and test datasets with these same values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "88c524dd5b0ae3730c55ef7776b586a3",
          "grade": false,
          "grade_id": "cell-d19928c9fc4333e3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "ip97FNz-jQP9"
      },
      "outputs": [],
      "source": [
        "x_mean = None\n",
        "x_std = None\n",
        "x_train_norm = None\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "383d686c570ffe3a7662ed4503080ae2",
          "grade": true,
          "grade_id": "normalize-data",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "02_NQzrAjQP9"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT MODIFY THIS CELL\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "32e1b80431dc9536616cd33e89f55eb9",
          "grade": false,
          "grade_id": "cell-754fc49b93ee7253",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "INpkjk1NjQP9"
      },
      "source": [
        "Now normalize the `x_validation` and `x_test` dataset, using the values of `x_mean` and `x_std` you have just calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e49b7a740af9ea62d56da4ddbae91fac",
          "grade": false,
          "grade_id": "cell-91fdda57170711e4",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "_YL_TmvKjQP-"
      },
      "outputs": [],
      "source": [
        "x_validation_norm = None\n",
        "x_test_norm = None\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fbd0f9f9ed71bb2d1a41c7478a99f0e5",
          "grade": true,
          "grade_id": "cell-402e87fe4175f349",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "XknnnquujQP-"
      },
      "outputs": [],
      "source": [
        "\"\"\" DO NOT MODIFY THIS CELL \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "391ad3ca8d1656c76d9d5e2d9e546a44",
          "grade": false,
          "grade_id": "cell-9fde81ca4d299f4a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "CE6Xt0D4jQP-"
      },
      "source": [
        "**Question:** Why is it important to (z-)normalize the input data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "99f8b0b9a8c099af6a5200ddfacce895",
          "grade": true,
          "grade_id": "why-normalize",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7I0BO74tjQP_"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8381ef0ae465af02cc9d83c40f3f6fa7",
          "grade": false,
          "grade_id": "cell-4b81a6d282c421be",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "H4Oot6LPjQP_"
      },
      "source": [
        "**Shuffle your training set!** This is necessary when we want to train a neural network using stochastic (mini-batch) gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3e12039604276db5da700590d320b531",
          "grade": false,
          "grade_id": "cell-76c83e890cfd73f3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "iL23w2tnjQQA"
      },
      "outputs": [],
      "source": [
        "indx=list(range(x_train.shape[0]))\n",
        "print(y_train)\n",
        "np.random.shuffle(indx)\n",
        "x_train_norm = x_train_norm[indx]\n",
        "y_train = y_train[indx]\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9385ebf3f27b8a3edb918500830c465f",
          "grade": false,
          "grade_id": "cell-36002411af6e0aa4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "hb0vbWqejQQA"
      },
      "source": [
        "**Question:** Why do you think it is important to shuffle the dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3eb4ae0f8b9717cfc2c8d54b0542fa2c",
          "grade": true,
          "grade_id": "why-shuffle",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "uKZ_6BlHjQQB"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9b9cb4b97761f4af58aca34480408033",
          "grade": false,
          "grade_id": "cell-491c1aa6dd52544a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Xda-OnULjQQB"
      },
      "source": [
        "**Question:** Whilst most high level generic frameworks allow you to split a dataset into training and validation and will also shuffle the data for you, they typically do not take into account extra constraints specifically required by medical imaging data. One of these is class imbalance, which we have briefly seen. In generating a training and validation set for medical imaging from a given dataset, there is another important criterion for determining which data goes into the training and which into the validation set, which is related to the actual patient. \n",
        "What is it and why is it important?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a388b52650b80c5f03a6072765ad9d2d",
          "grade": true,
          "grade_id": "no-patient-overlap",
          "locked": false,
          "points": 4,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Q8UkcdMsjQQB"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2d1f90cdbe6d05c3fde142891697bc06",
          "grade": false,
          "grade_id": "cell-458853ed06337e42",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "5za_e1byjQQC"
      },
      "source": [
        "### kNN classifier\n",
        "\n",
        "Now that we have defined a training and a validation set, we can define a baseline result by applying kNN classifier (which we have seen in previous assignments), and compute the accuracy on the validation set. If you have made a balanced validation set, accuracy is a good evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "223e9d47c90c9772e6320b6bdb3e54ab",
          "grade": false,
          "grade_id": "cell-cef3795503db850c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "2Z8MHDibjQQC"
      },
      "outputs": [],
      "source": [
        "# kNN\n",
        "classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=30)\n",
        "classifier.fit(x_train_norm, y_train)\n",
        "y_validation_auto = classifier.predict(x_validation_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "018f9ccc8a9a2839485981174ae25c2f",
          "grade": false,
          "grade_id": "cell-24611645b9858e3c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4Gr6pY6ljQQC"
      },
      "source": [
        "Compute the confusion matrix for the results with kNN classifier. In order to compute the **confusion matrix** and the **accuracy**, you can use the following functions from the [scikit-learn library](http://scikit-learn.org/stable/documentation.html):\n",
        "\n",
        "* ```sklearn.metrics.confusion_matrix()```\n",
        "* ```sklearn.metrics.accuracy_score()```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "40e861bd1587f6b34d328f363fb131d5",
          "grade": false,
          "grade_id": "cell-edba12d7689b01c2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "vzfBRTo_jQQD"
      },
      "outputs": [],
      "source": [
        "# Replace None with your code\n",
        "# confusion matrix\n",
        "conf_mat_knn = None\n",
        "# accuracy\n",
        "acc_knn = None\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "57bdbc7a7f9a0d0db07abd1c8ca4245f",
          "grade": true,
          "grade_id": "conf-matrix-acc-test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "VyytKkNkjQQD"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT MODIFY THIS CELL\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a1b28a0df08c9953eab2a9607e117247",
          "grade": false,
          "grade_id": "cell-96792216036dc2f7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "NWjt5ToejQQE"
      },
      "outputs": [],
      "source": [
        "print(f'Accuracy using kNN: {100.0*acc_knn:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0ce0591699e9355672bdc2d4eb5e4770",
          "grade": false,
          "grade_id": "cell-0311247149d4bdab",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "eHIaYvUcjQQE"
      },
      "source": [
        "You can use the following convenience function to visualize the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "15895adbdd754768a9fc0757060c4a79",
          "grade": false,
          "grade_id": "cell-2e9519601b8a12ac",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "l92Tzs9yjQQE"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(conf_mat, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix\n",
        "    \"\"\"\n",
        "    plt.imshow(conf_mat, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    thresh = conf_mat.max() / 2.\n",
        "    for i, j in itertools.product(range(conf_mat.shape[0]), range(conf_mat.shape[1])):\n",
        "        plt.text(j, i, conf_mat[i, j], horizontalalignment=\"center\",\n",
        "                 color=\"white\" if conf_mat[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "eb4e4e2706cbd61d76076094dfc9ec84",
          "grade": false,
          "grade_id": "cell-97588c50ebd19aa8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "0RryQnLqjQQF"
      },
      "outputs": [],
      "source": [
        "# visualize the confusion matrix\n",
        "plot_confusion_matrix(conf_mat_knn, classes=noduleTypes,\n",
        "                      title='Confusion matrix: kNN as classifier (True label vs. Predicted label)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6efd45bab649f7f91ee2b66c2daf63e7",
          "grade": false,
          "grade_id": "cell-b0fc97028d1724a7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5XGY90ywjQQF"
      },
      "source": [
        "The confusion matrix highlights a problem of the kNN classifier as it is used now. Can you find it and explain why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "04b532664376c069b87c232d9a3f5dc1",
          "grade": true,
          "grade_id": "conf-matrix-knn-problem",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "aETeCV41jQQG"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d27b53fc76378e202ae3de35e4aab5c1",
          "grade": false,
          "grade_id": "cell-635a0df4117c9914",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "qz7mrkjPjQQG"
      },
      "source": [
        "# Classification with Neural Networks\n",
        "Now that some kind of baseline result has been obtained with kNN, we can start developing a classifier based on neural networks.\n",
        "For this purpose, we will use the the [**PyTorch**](https://pytorch.org/) library, which implements classes and functions that make building and training neural networks easy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d6af015f99387b43009192f548c84589",
          "grade": false,
          "grade_id": "cell-52d15dfeeeb9b4ce",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1ZARPBwAjQQH"
      },
      "source": [
        "## Building a two-layer Neural Network to classify nodule features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c90e9b049b6f264133dc0cc6c90338f7",
          "grade": false,
          "grade_id": "cell-15aa1d0d47cc393e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "IyvAfDNRjQQI"
      },
      "source": [
        "<img src=\"https://github.com/ivanslootweg/ISMI/blob/main/assignment_3/figures/learning_framework.png?raw=1\" alt=\"Learning framework\" align=\"right\" width=\"350\">\n",
        "\n",
        "In this first part of the assignment, we are going to build a neural network with two hidden layers in PyTorch. For this particular assignment we are going to use the fully connected layers, also called Linear layers in PyTorch (visit the following link to get more details of default parameters https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear).\n",
        "\n",
        "As we have seen in the lecture this week, in order to build our classification framework with neural networks, we have to define and specify three main components:\n",
        "\n",
        "1. Network architecture\n",
        "2. Loss function\n",
        "3. Optimization algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "14b2fb17041be69df184759f7c3330aa",
          "grade": false,
          "grade_id": "cell-73708b247880254d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "MQSycYl8jQQI"
      },
      "source": [
        "### Creating the neural network\n",
        "\n",
        "We will build this first network to have **two hidden layers of 15 and 8 neurons respectively**.\n",
        "Later, we can experiment with a different number of neurons, hidden layers, etc., but let's start with this one.\n",
        "\n",
        "Keep in mind that the size of the input and of the output layer of your network are given by the data and the classification problem you have to solve.\n",
        "Therefore, before you start building the network, it is good to check the dimensionality of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c20189b6e3b66bb1571f524cc0c61a64",
          "grade": false,
          "grade_id": "cell-c75aa716a7699a6c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "S5u6_1JMjQQJ"
      },
      "outputs": [],
      "source": [
        "data_size = x_train_norm.shape\n",
        "n_classes = len(noduleTypes)\n",
        "print(f\"Data size: {data_size}\")\n",
        "print(f\"Number of classes: {n_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "67717334610dbbfc81d3b37c8b1fcf23",
          "grade": false,
          "grade_id": "cell-82ac076333fe5878",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "s1SEETTqjQQJ"
      },
      "source": [
        "Define the architecture of a neural network with two hidden layers of 15 and 8 neurons respectively.\n",
        "In your implementation, consider what follows:\n",
        "\n",
        "* You can take inspiration from the PyTorch beginner tutorials, for example this page: https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
        "\n",
        "* In PyTorch, you define a neural network as a Python class, which inherits from PyTorch's basic `nn.Module`. Classes are a feature of [object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming). If you are not familiar with that, don't worry! Follow the template below in the assignment and everything should work out. Otherwise ask a TA for help!\n",
        "\n",
        "* Use the *ReLU* activation function in between the fully connected layers. All activation functions can be found [here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).\n",
        "\n",
        "* Use the softmax activation function for the output of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ba863026f3681a7f6cc9cd2a151c627b",
          "grade": true,
          "grade_id": "build-simple-network",
          "locked": false,
          "points": 10,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "rOXlVwwgjQQK"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, data_size, num_classes):\n",
        "        \"\"\"Simple neural network with two hidden layers\n",
        "        \n",
        "        data_size: tuple\n",
        "            Defines the input data size as (num_samples, num_features)\n",
        "        num_classes: int\n",
        "            Defines the number of classes\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_layer_1 = None\n",
        "        self.relu_1 = None\n",
        "        ...\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.hidden_layer_1(x)\n",
        "        out = self.relu_1(out)\n",
        "        ...\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9bfe9fb668d05aabf611712bd24f6ee9",
          "grade": false,
          "grade_id": "cell-39c6663aac2143fa",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SSMvicYOjQQK"
      },
      "source": [
        "Now, you can instantiate the class that you have just coded to build your neural network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7b4ca3cfcf39af12ff94fcbec16d4b26",
          "grade": false,
          "grade_id": "cell-e681cd9433fb1526",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "KSwY9HUYjQQL"
      },
      "outputs": [],
      "source": [
        "# build neural network object\n",
        "network=NeuralNetwork(data_size, n_classes)\n",
        "print(network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0b51af1f0bc2e28170176b4fc7e0c2fc",
          "grade": false,
          "grade_id": "cell-7b6b82f63d0dc89e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "x5Vbb3IBjQQL"
      },
      "source": [
        "## Loss Function & Optimization Algorithm\n",
        "\n",
        "Now that the architecture is defined, we have to specify the two other components of our learning framework, namely the loss function and the optimization algorithm.\n",
        "\n",
        "### Loss\n",
        "We have to define a function that, given the network, gets the predicted probability for a given input sample.\n",
        "Since we are dealing with a multi-class classification problem, **categorical cross-entropy** seems a reasonable choice. Since our network already implements the final softmax layer, we use [NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss).\n",
        "\n",
        "### Optimization algorithm\n",
        "We also have to specify how we want to train our model. In our case, we will use \"Stochastic Gradient Descent\". As we have seen in the lecture this week, gradient descent algorithms need a **learning rate**, which indicates how much we step in the (opposite) direction of the gradient. We have also seen that strategies to adapt the learning rate during training are possible, but for the moment we just define a fixed learning rate. Pick a value and see what happens, you can optimize this later.\n",
        "\n",
        "We set the loss function, learning rate and optimizer in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Bzt__2QkjQQM"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.NLLLoss()\n",
        "learning_rate = 0.0001 # pick a value for your learning rate, we suggest 0.0001 as a starting point\n",
        "sgd = optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate) # define Stochastic Gradient Descent as the optimizer, which takes the learning rate as input parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZXSKZqLjQQM"
      },
      "source": [
        "### Metrics\n",
        "Since we are developing a classifier for a multi-class problem, accuracy is a reasonable choice.\n",
        "\n",
        "In the next cell, define a function that evaluates the loss and the accuracy given `model`, `x` and `y`. You can use the second cell to evaluate your raw initialized model, to check if everything runs the way you expect. (What initial accuracy do you expect?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c4e305c2f96d7591eb76cdac3813efa3",
          "grade": false,
          "grade_id": "cell-718cc01c4251c5a0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "brVbVwC6jQQM"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, x, y):\n",
        "    \"\"\"Evaluate the loss and accuracy of a PyTorch model\n",
        "    \n",
        "    model: PyTorch nn.Module\n",
        "    x: numpy array\n",
        "        Input features (n_samples, n_features)\n",
        "    y: numpy array\n",
        "        Ground truth class (n_samples, 1)\n",
        "    \"\"\"\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    x = torch.tensor(x, dtype=torch.float)  # Model input should be `float`\n",
        "    y = torch.tensor(y, dtype=torch.long)  # NLLLoss expect the target (y) to be `long`\n",
        "    \n",
        "    # Predict with the model\n",
        "    y_pred = None\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # Calculate the loss\n",
        "    loss_value = None\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    y_pred_class = None  # Should be same shape as y and contain class predictions for each sample\n",
        "    accuracy = None\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    return loss_value, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "krUQpOv-jQQN"
      },
      "outputs": [],
      "source": [
        "# Calculate loss and accuracy for the untrained model\n",
        "loss_value, accuracy = evaluate_model(network, x_validation_norm, y_validation)\n",
        "print(f\"Initial validation accuracy = {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "012a183b9f89a4f53af0d67db94c8984",
          "grade": true,
          "grade_id": "cell-94eb2baa9bba5329",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "DGbdZjNxjQQN"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT MODIFY THIS CELL\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ba2c9dbbdf82d34cb8fe4785b01fd09b",
          "grade": false,
          "grade_id": "cell-fbd0cbaf9e4385b5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "cONMhLMvjQQO"
      },
      "source": [
        "### Learning procedure\n",
        "Now we can write the learning algorithm, as we have seen in the lecture this week.\n",
        "Basically, we will iteratively update the parameters of our network by extracting mini-batches from the training set, until all the training samples have been used. After all training samples have been seen by the network once, one epoch is done. We repeat this procedure for a number of epochs that you define.\n",
        "During the training loop, we also want to check the performance of the trained network on the validation set.\n",
        "Therefore, for each epoch, after a training pass, we also classify the validation set.\n",
        "\n",
        "We provide the main structure of the learning script, you must implement the missing parts:\n",
        "\n",
        "* Train the network\n",
        "* Get the training loss and accuracy\n",
        "* Validate your network on the validation data\n",
        "* Get the validation loss and accuracy\n",
        "\n",
        "During training, we will also be saving to disk the parameters of the network which has the best performance on the validation set. This will be stored as the file ```best_model.pt``` in the directory ```network_dir```, which by default is the root directory of this notebook.\n",
        "\n",
        "### Implementation\n",
        "\n",
        "The function `train_network` is defined below, which takes the training and validation dataset and `max_epochs` as parameters. A lot of the \"routine\" code has been implemented already. Please replace the items marked as None with correct values. Have a look at the [PyTorch tutorials](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) to find out how to do a backpropagation and optimization step "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4bb7c7ec6c58314e67958d9e80c79226",
          "grade": true,
          "grade_id": "cell-bc0b215136eab0da",
          "locked": false,
          "points": 10,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "vvMb-TZHjQQP"
      },
      "outputs": [],
      "source": [
        "def train_network(model, x_train, y_train, x_val, y_val, max_epochs: int):\n",
        "    # lists where we will be storing values during training over all epochs (for visualization purposes later)\n",
        "    train_loss_list = []\n",
        "    train_accuracy_list = []\n",
        "    val_loss_list = []\n",
        "    val_accuracy_list = []\n",
        "    \n",
        "    network_filepath = network_dir / 'best_model.pt'\n",
        "\n",
        "    # we want to save the parameters that give the best performance on the validation set\n",
        "    # therefore, we store the best validation accuracy, and save the parameters to disk\n",
        "    best_val_accuracy = 0 # best validation accuracy\n",
        "\n",
        "    for epoch in tqdm(range(max_epochs), position=0, leave=False, desc=\"Epochs\"):\n",
        "        # Loop over all samples in the training set\n",
        "        for x, y in tqdm(zip(x_train, y_train), position=1, leave=False, desc=\"Samples\", total=x_train.shape[0]):\n",
        "            # Add batch dimension\n",
        "            x = x[None, ...]\n",
        "            y = y[None, ...]\n",
        "            \n",
        "            # Calculate the loss on the training sample (hint: use the evaluate_model function)\n",
        "            loss = None\n",
        "            # YOUR CODE HERE\n",
        "            \n",
        "            # Optimization step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        # Calculate training and validation metrics\n",
        "        train_loss = train_accuracy = None\n",
        "        val_loss = val_accuracy = None\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        # Store values for plotting\n",
        "        train_loss_list.append(float(train_loss))\n",
        "        train_accuracy_list.append(train_accuracy)\n",
        "        val_loss_list.append(float(val_loss))\n",
        "        val_accuracy_list.append(val_accuracy)\n",
        "            \n",
        "        # Update best validation accuracy and save the network to `network_filepath`\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = None\n",
        "            # Look up how to save your network!\n",
        "            # YOUR CODE HERE\n",
        "            \n",
        "    return train_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "T9d8jhvjjQQQ"
      },
      "outputs": [],
      "source": [
        "train_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list = train_network(network, x_train_norm, y_train, x_validation_norm, y_validation, max_epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "eWzUmRGajQQQ"
      },
      "outputs": [],
      "source": [
        "# Visualization of the learning curves\n",
        "fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "axes[0].plot(range(len(train_loss_list)), train_loss_list)\n",
        "axes[0].set_title(\"Training loss\")\n",
        "axes[0].set_xlabel(\"Training step\")\n",
        "\n",
        "axes[1].plot(range(len(train_accuracy_list)), train_accuracy_list)\n",
        "axes[1].set_title(\"Training accuracy\")\n",
        "axes[1].set_xlabel(\"Training step\")\n",
        "\n",
        "axes[2].plot(range(len(val_loss_list)), val_loss_list)\n",
        "axes[2].set_title(\"Validation loss\")\n",
        "axes[2].set_xlabel(\"Training epoch\")\n",
        "\n",
        "axes[3].plot(range(len(val_accuracy_list)), val_accuracy_list)\n",
        "axes[3].set_title(\"Validation accuracy\")\n",
        "axes[3].set_xlabel(\"Training epoch\")\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2cb43ac5deb9f41ef307dedaf4112497",
          "grade": false,
          "grade_id": "cell-1c8428ef60cd3f93",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ZSOfXY8VjQQR"
      },
      "source": [
        "Now call your function to train the network, finally!\n",
        "\n",
        "Feel free to experiment with different values of `max_epochs`:\n",
        "    \n",
        "  * max_epochs: define max_epochs to give a reasonable number of complete training iterations but that will not take (seemingly) forever to complete: 50... 100 .. 150 ... 200 are typical ranges\n",
        "\n",
        "### FOR FINAL VALIDATION / SUBMISSION\n",
        "\n",
        "Please ensure you call `train_network()` with `max_epochs` set to `20` before submitting this notebook for grading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "IgQ_aqkLjQQR"
      },
      "outputs": [],
      "source": [
        "_, _, _, _ = train_network(network, x_train_norm, y_train, x_validation_norm, y_validation, max_epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8a4ecab99b36c330745c0afd1f0165c2",
          "grade": false,
          "grade_id": "cell-b7e714728aacbf2d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "tZ6edH7xjQQR"
      },
      "source": [
        "## Classification: validation set\n",
        "Now we can use the trained network to classify the validation set (again), and check that the performance corresponds to the best value obtained during training. We can compute the accuracy and also visualize the confusion matrix, to get a feeling how well we are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4f102d242954efbabb281a88d65edf80",
          "grade": false,
          "grade_id": "cell-71701b8d4fd537a1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "UYQmMD73jQQR"
      },
      "outputs": [],
      "source": [
        "network_filepath = network_dir / 'best_model.pt'\n",
        "\n",
        "# Re-load the model found for the best accuracy\n",
        "# the function 'load_model' takes care of compiling again the function \n",
        "best_network = torch.load(network_filepath)\n",
        "best_network.eval()\n",
        "\n",
        "# Calculate the prediction on the validation set using the best network\n",
        "y_val_pred = None\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Assess the confusion matrix to check if the performance corresponds to the best value obtained during the training\n",
        "conf_mat_nn = None\n",
        "acc_nn = None\n",
        "# YOUR CODE HERE\n",
        "print(f\"Accuracy on validation set: {acc_nn:.2f}\")\n",
        "plot_confusion_matrix(conf_mat_nn, classes=noduleTypes,\n",
        "                      title='Confusion matrix: Neural Network classifier (True label vs. Predicted label)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bb4f77945ea9c4fce64de3f8c97e7ef9",
          "grade": true,
          "grade_id": "cell-8c13a3f9cdcc00a5",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "VmHrC4fHjQQR"
      },
      "outputs": [],
      "source": [
        "\"\"\" DO NOT MODIFY THIS CELL \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "72a4f34da296466331738465cec22368",
          "grade": false,
          "grade_id": "cell-a4d2243049e221f4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "t7Clqa2LjQQR"
      },
      "source": [
        "Use the labels in the validation set to identify the cases that you are misclassifying, and see what the network predicts for those cases. Since the labels in our dataset are given by humans (no ground truth available, only reference standard), there can be some confusion in the way nodules are classified, even in the reference standard.\n",
        "\n",
        "Based on what you have learned about the appearance of nodules at the beginning of this notebook, do you think you agree with the labels predicted by your network? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "044b72a4fd5f69ce0fa4cb698e2357e0",
          "grade": true,
          "grade_id": "cell-eadf114636721c33",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Mdh9pCI2jQQS"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cef6fd9d657c2753a86a21e7ae72a12b",
          "grade": false,
          "grade_id": "cell-3d14c83e5349d5e1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Jl5THPn_jQQS"
      },
      "source": [
        "## Classification: test set\n",
        "Now we can repeat the classification step on the test set, and submit the results to grand-challenge.org. During the test procedure, we will save the predictions in a csv file, which you can submit.\n",
        "Please note that the reference standard in grand-challenge has labels y = [1, ..., 5]. Take this into account when making the csv file for your submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c2815ba84b576e1d0c334921ce7982d9",
          "grade": false,
          "grade_id": "cell-c31941f033be751f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "XSxWBlBQjQQS"
      },
      "outputs": [],
      "source": [
        "# Classify the test set\n",
        "\n",
        "# Create a .csv file for saving the results of the classification\n",
        "with open('results.csv', 'w') as h_csv:\n",
        "    h_csv.write('nodule_id,label\\n')\n",
        "    \n",
        "    # For each sample within the test set, get the prediction from the network and save it \n",
        "    n_test_samples = x_test_norm.shape[0]\n",
        "    for idx, x in enumerate(x_test_norm):\n",
        "        x = x[None, ...]\n",
        "        x = torch.tensor(x, dtype=torch.float)\n",
        "        \n",
        "        # Get its nodule id\n",
        "        nodule_id = nodule_ids_test[idx]\n",
        "        \n",
        "        # Make prediction using neural network\n",
        "        prediction = best_network(x)\n",
        "        prediction = np.argmax(prediction.detach().numpy(), axis=1)[0]\n",
        "\n",
        "        # Write the label to file\n",
        "        h_csv.write(f\"{nodule_id.decode('UTF-8')},{prediction+1}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3f71ae804178b34b156506b424545c2b",
          "grade": false,
          "grade_id": "cell-7cb93439ee39a77c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "nPofbIIZjQQS"
      },
      "source": [
        "## Submit your results\n",
        "\n",
        "You can now download the `results.csv` file, using the file explorer on the left.  \n",
        "Next, upload your result to the challenge website (https://ismi-nodules.grand-challenge.org/) and see how well you performed compared to your fellow students! You can submit as often as you want, only the best result counts.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "eaa01ed8bc8a6f9b7b362cb4c6f70b19",
          "grade": false,
          "grade_id": "cell-348ef8aee5dcfa81",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "zdr_G4JAjQQS"
      },
      "source": [
        "# Task 2: Improve your neural network\n",
        "So far, we have implemented a simple network with two hidden layers of 15 and 8 neurons respectively, and a relu activation function.\n",
        "We also used a constant learning rate.\n",
        "This means that there is plenty of room for improvement!\n",
        "You can, for example:\n",
        "* change the architecture of your network, add neurons, add layers\n",
        "* change the activation function, try e.g. sigmoid and see what happens\n",
        "* change the learning rate, or try to find a strategy to adapt it during training\n",
        "\n",
        "In order to fine-tune these parameters, you may want to expand the main script used for training and validation to include a search for the optimal set of hyper-parameters (for example, cross-validation).\n",
        "For each experiment:\n",
        "* provide a clear description of the setup (value, range of parameters used in the search), which we can read and understand\n",
        "* save the trained network, which you can load later\n",
        "\n",
        "In particular, show and explain:\n",
        "* how the results change by changing the learning parameters/architecture of the network\n",
        "* how the learning rate worked and how it affected the performance of the neural network\n",
        "\n",
        "Note that hyper-parameters tuning has to be done using the **validation** set.\n",
        "When you are happy with the performance on the vaidation set, you can submit the results to grand-challenge!\n",
        "\n",
        "**This may sound like a boring task, but is actually what it takes to make good neural networks!!! Researchers do this very often!!!**\n",
        "\n",
        "Do not modify previous cells in this notebook, and do not create any new cells, as this may interfere with grading. We have added plenty of cells below, so you should not run out of space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bfe2271bec7fd64bb4d524a89477e65f",
          "grade": false,
          "grade_id": "cell-5fd508b9c76c8d92",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "NhIyvka4jQQS"
      },
      "source": [
        "### Step 1. Define your improved neural network\n",
        "\n",
        "Your class may look something like the following (similar to the function you wrote earlier in the assignment to build a neural network):\n",
        "\n",
        "```python\n",
        "class ImprovedNeuralNetwork(nn.Module):\n",
        "    def __init__(self, data_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.hidden_layer_1 = ...\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden_layer_1(x)\n",
        "        x = ...\n",
        "        return x\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "366c395a85b3ae72f3e66f3c8479f7a0",
          "grade": true,
          "grade_id": "cell-5562751609d9cd82",
          "locked": false,
          "points": 6,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "uQnGKH2EjQQT"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FQax4zujQQT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOY90Ai0jQQT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zxkWQeUjQQT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAYullg6jQQU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "de7f8b0e4f9ab81ce10b951929c211a7",
          "grade": false,
          "grade_id": "cell-13bee7eb412f955a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "gdTOvW3OjQQU"
      },
      "source": [
        "### Step 2. Optimizer, Loss and Learning Rate\n",
        "\n",
        "Specify the optimizer, loss and learning rate as you did earlier. At this point you may wish to try e.g. a different optimizer to the SGD optimizer used earlier.\n",
        "Hint: the Adam optimizer might be an interesting choice ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2F5Y3najQQU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II_B1oQYjQQU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPp7eR8xjQQV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx2Tjl8njQQV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ba017034e5d4944201a54b41a6789c4c",
          "grade": true,
          "grade_id": "cell-0c564e1810f00bca",
          "locked": false,
          "points": 6,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "607b9_MhjQQV"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ed8d3ebf38ea7a56011efdd5da2b8416",
          "grade": false,
          "grade_id": "cell-8fe6194c09c73189",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7kIrL6L1jQQV"
      },
      "source": [
        "### Step 3. Train your improved network\n",
        "\n",
        "So this is where you bring it all together: data loading and training epochs. To see how well the training works on your validation dataset, keep track of metrics such as `loss` and `accuracy` and maybe even plott the metrics so you can see how the training is progressing.\n",
        "\n",
        "Good luck - and above all enjoy doing some exploration..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuRIoyQJjQQV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz5hkq1ljQQW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBPGjPbgjQQW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ouk0XewcjQQW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8586ad6b1acae74241f2e9292cde4331",
          "grade": true,
          "grade_id": "cell-7cef5f74293c1c6f",
          "locked": false,
          "points": 8,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "2BWyHKsRjQQW"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2c01d0e8923927c6f35b9b647fe30cf3",
          "grade": false,
          "grade_id": "cell-d6df98b2f3e39e6c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "GBrdkTAajQQX"
      },
      "source": [
        "# Task 3: Train a neural network with raw nodule data\n",
        "Now that you have developed your supervised learning framework using the features that we provided, repeat the procedure using raw data as input. You can use the functions provided at the beginning of this notebook to extract 2D views from 3D nodules, which could be useful to develop your network.\n",
        "\n",
        "You will have to define functions yourself that load the data from `LIDC-IDRI/{training,test}/nodules/` yourself. For the training data, you can assign the class number (0-4) using the directory names inside the `nodules` directory.\n",
        "\n",
        "Once patches have been extracted, you can use a strategy similar to what showed in the lecture this week (when a linear model was used to classify CIFAR10 images) and vectorize patches to obtain feature vectors.\n",
        "\n",
        "Repeat the training procedure, tune the hyper-parameters, and submit the new results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cdffa5c4e100d43c3c04703019a4514e",
          "grade": false,
          "grade_id": "cell-563fb7f9edb44a15",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "xEwIp4R9jQQX"
      },
      "source": [
        "## Loading Train Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSXTyJxXjQQX"
      },
      "source": [
        "### Step 1. Data Loading. \n",
        "\n",
        "Define a function that loads and preprocesses the raw data in the cell below (copy + paste + modify from code above is not only allowed but actively encouraged :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "240c64b7e577b66597f74a80be806131",
          "grade": true,
          "grade_id": "cell-683d17e84dce0ea9",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Y86Ir7lkjQQX"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f2G89OqjQQX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGvqfhP9jQQY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkSFQQYLjQQY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXw-TOyljQQY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1a70ce72ee6e0830b07a4208b831d686",
          "grade": false,
          "grade_id": "cell-ec3daa77215123a6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "eiuE8NcHjQQY"
      },
      "source": [
        "### Step 2. Build the neural network\n",
        "\n",
        "Define a function (or functions) that creates your neural network in the cell below (hint: copy + paste + modify the function you wrote above to create a neural network). Remember to specify the learning rate, optimizer and loss functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ad5b0eb3d47fbbc7c5785ff7e8300a90",
          "grade": true,
          "grade_id": "cell-9d5e7d3931a0cdb3",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "jdNXSQADjQQY"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8nM34KdjQQZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NIk6jkfjQQZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkVV1qwajQQZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2QGA7OfjQQZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c3dca80f033f1f45725254a8237d5eb0",
          "grade": false,
          "grade_id": "cell-1a4e5fee0a64a04a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "E9JI-8iAjQQZ"
      },
      "source": [
        "### Step 3 Training\n",
        "\n",
        "Write your own training loop (as you did above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "09a4dd41e009878380a5d5e59e7a90f3",
          "grade": true,
          "grade_id": "cell-c257006e1e9b5844",
          "locked": false,
          "points": 4,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "OOS3yfd9jQQZ"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWoWXw3sjQQa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WLlZLu3jQQa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUPZtivhjQQa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlWOZZe8jQQa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc-showmarkdowntxt": false,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}