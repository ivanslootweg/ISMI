{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Systems in Medical Imaging 2023\n",
    "\n",
    "This Jupyter notebook is part of the course Intelligent Systems in Medical Imaging (ISMI) from Radboud University (Nijmegen, Netherlands), and it was developed by researchers of Radboud University Medical Center (Nijmegen, Netherlands).\n",
    "\n",
    "You should have obtained this notebook by downloading it from the official Brightspace page of the course.\n",
    "\n",
    "This notebook formulates an assignment as part of the ISMI course, and the content of this notebook should be used solely to develop a solution to this assignment. You should not make the code provided in this notebook, or your own solution, publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teaching Assistants\n",
    "\n",
    "* Bram de Wilde (bram.dewilde@radboudumc.nl)\n",
    "* Pierpaolo Vendittelli (pierpaolo.vendittelli@radboudumc.nl)\n",
    "* Joeran Bosma (joeran.bosma@radboudumc.nl)\n",
    "* Stephan Dooper (stephan.dooper@radboudumc.nl)\n",
    "\n",
    "For questions about the assignments that go beyond the content, you can contact Bram de Wilde. Questions about the content are addressed in the tutorial sessions. You are also encouraged to use the Brightspace discussion forums to discuss content of the assignments. We will also keep an eye out there to help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines and instructions\n",
    "Make sure you write code in any place that says \"YOUR CODE HERE\" by substituting `None` variables or by adding your own solution. Make sure you write in any place that says \"YOUR ANSWER HERE\" with your answers to the questions.\n",
    "\n",
    "Please **do not delete or add any cells**.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs without errors. The easiest way to check this is to restart the kernel and run all cells (in the menubar, select Runtime$\\rightarrow$Restart & Run All).\n",
    "\n",
    "* Groups: You should work in **groups of 2 or 3 people**. (groups of 2 are preferred!)\n",
    "* You are expected to work in Google Colab. If you run the notebooks locally, you may have to solve some issues yourself!\n",
    "* Submit your **fully executed** notebook to Brightspace with file name format: `GroupN_NameSurname1_NameSurname2_NameSurname3.ipynb`\n",
    "* The deadlines for all assignments are on Brightspace.\n",
    "* Deadlines are soft, but make an effort to be on time. We prioritise feedback on assignments that are handed in before the deadline.\n",
    "* Each assignment has 100 points, your grade is your total number of points divided by 10.\n",
    "* The assignments are mandatory, but **do not count** towards your final grade for the course.\n",
    "* For assignments where you have to submit to grand-challenge.org, use team name format `ismi-GroupN-nickname1`.\n",
    "* When working with Google Colab, we advise you to download model checkpoints (.h5 files). This way you don't lose your checkpoint if your session times out. Also, don't forget to connect to a **GPU runtime** when training neural networks!\n",
    "* In Google Colab, you can mount your Google Drive to save files, by clicking the Folder icon on the left, and then click the Mount Drive icon.\n",
    "\n",
    "There are more detailed instructions on Brightspace on how to use Google Colab for the assignments. You can find it here: https://brightspace.ru.nl/d2l/le/content/333312/Home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "Please fill in this cell with your names, e-mail address and s-numbers. This information will be used to grade your assignment.\n",
    "\n",
    "* [Name student #1], [s-number], [e-mail address]\n",
    "* [Name student #2], [s-number], [e-mail address]\n",
    "* [Name student #3], [s-number], [e-mail address]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7330907204d5404674315996c4031de",
     "grade": false,
     "grade_id": "cell-e438c2b9fedda311",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 6: Lung detection in chest X-ray\n",
    "\n",
    "<img src=\"figures/xray.jpeg\" width=\"200\" height=\"200\" align=\"right\">\n",
    "\n",
    "In this assignment, you will develop a system to automatically detect the lungs in a chest X-ray scan.\n",
    "For this task you will use a single-shot network called [YOLO](https://arxiv.org/pdf/1506.02640.pdf) (You Only Look Once). Details on the YOLO (and the more recent YOLO9000) method can be found in the following papers:\n",
    "\n",
    "  1. **You Only Look Once: Unified, Real-Time Object Detection** [https://arxiv.org/abs/1506.02640]\n",
    "  2. **YOLO9000: Better, Faster, Stronger** [https://arxiv.org/abs/1612.08242] \n",
    "\n",
    "Chest X-ray is the most commonly acquired image in medicine. Chest X-ray uses a very small dose of ionizing radiation to produce pictures of the inside of the chest. It is used to evaluate the lungs, heart and chest wall and may be used to help diagnose shortness of breath, persistent cough, fever, chest pain or injury. It also may be used to help diagnose and monitor treatment for a variety of lung conditions such as pneumonia, emphysema and cancer. Because chest X-ray is fast and easy, it is particularly useful in emergency diagnosis and treatment.\n",
    "\n",
    "Because of the difference in density between air, soft tissue and bone, the lungs appear much darker than their surroundings. Brighter regions in the lungs may indicate the presence of pathology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40f375e1e79af29a432d889027cc615f",
     "grade": false,
     "grade_id": "cell-99d074af24a33c77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Tasks for this assignment \n",
    "\n",
    "The goal of this assignment is to get familiar with the YOLO architecture, loss function and training procedure, as well as the type of output it produces and how to transform it into actual predictions of bounding boxes. Additionally, you will get familiar with the problem of detecting lungs in chest X-ray. The models here could be re-trained in the future to detect more anatomical structures in chest X-ray images.\n",
    "In order to get started with the YOLO architecture, we will first detect only the **right lung**, and once we are familiar with the whole training pipeline, then re-train to detect both left and right lungs.\n",
    "\n",
    "The three main tasks of this assignment are:\n",
    "\n",
    "* Task 1: Decode the network output to visualize the predicted bounding boxes for the right lung\n",
    "* Task 2: Retrain the network on both lungs\n",
    "* Task 3: Improve performance with data augmentation or alteration of the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6864bbfd2012a9de5515a0ad28da9c8",
     "grade": false,
     "grade_id": "cell-30e14d48e4c62fa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "The data used in this assignment belong to the **CHESTXRAY14** dataset, which is publicly available and can be found at this link: https://nihcc.app.box.com/v/ChestXray-NIHCC. The dataset, released by the NIH, contains 112,120 frontal-view X-ray images of 30,805 unique patients, annotated with up to 14 different thoracic pathology labels using NLP methods on radiology reports.\n",
    "\n",
    "For this assignment, we have selected 13,331 chest X-ray images from CHESTXRAY14 and generated bounding boxes containing the left and the right lungs. The coordinates of bounding boxes are extracted from a previously obtained segmentation of the lungs, available in our research group (lung segmentation is not available in the CHESTXARY14 dataset), as depicted in the following example:\n",
    "\n",
    "<table width=\"100%\" border=\"0\">\n",
    "  <tr>\n",
    "  <td style=\"text-align:center\">chest x-ray image</td>\n",
    "  <td style=\"text-align:center\">lung segmentation</td>\n",
    "  <td style=\"text-align:center\">bounding box</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td><img src=\"figures/00000007_000.PNG\" alt=\"\" align=\"center\" /></td>\n",
    "  <td><img src=\"figures/00000007_000_segmentation.PNG\" alt=\"\" align=\"center\" /></td>\n",
    "  <td><img src=\"figures/00000007_000_overlay.PNG\" alt=\"\" align=\"center\"/></td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Bounding box data come in ```xml``` format, which is the format read by the YOLO training script that we will use in this assignment (more details about this in next cells).\n",
    "An example of the content of the ```xml``` file for bounding boxes of right and left lung for the image ```00000004_000``` is the following:\n",
    "\n",
    "```xml\n",
    "<annotation verified=\"no\">\n",
    "  <folder>Lungs</folder>\n",
    "  <filename>00000004_000</filename>\n",
    "  <source>\n",
    "    <database>Unknown</database>\n",
    "  </source>\n",
    "  <size>\n",
    "    <width>512</width>\n",
    "    <height>512</height>\n",
    "    <depth>1</depth>\n",
    "  </size>\n",
    "  <segmented>0</segmented>\n",
    "  <object>\n",
    "    <name>RL</name>\n",
    "    <pose>Unspecified</pose>\n",
    "    <truncated>0</truncated>\n",
    "    <difficult>0</difficult>\n",
    "    <bndbox>\n",
    "      <xmin>64</xmin>\n",
    "      <ymin>62</ymin>\n",
    "      <xmax>256</xmax>\n",
    "      <ymax>490</ymax>\n",
    "    </bndbox>\n",
    "  </object>\n",
    "  <object>\n",
    "    <name>LL</name>\n",
    "    <pose>Unspecified</pose>\n",
    "    <truncated>0</truncated>\n",
    "    <difficult>0</difficult>\n",
    "    <bndbox>\n",
    "      <xmin>298</xmin>\n",
    "      <ymin>78</ymin>\n",
    "      <xmax>450</xmax>\n",
    "      <ymax>502</ymax>\n",
    "    </bndbox>\n",
    "  </object>\n",
    "</annotation>\n",
    "```\n",
    "\n",
    "Among all the parameters that you can find there, the most important ones are the following:\n",
    "\n",
    "* ```(width, height)```: image size\n",
    "* ```(RL, LL)```: labels for Right Lung and Left Lung\n",
    "* ```(xmin, xmax, ymin, ymax)```: coordinates of the top-left and bottom-right corners of the bounding box\n",
    "\n",
    "It is important that to understand these parameters before you continue, because you will be using them in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14f915e45361c3246c205a6ef6a877f5",
     "grade": false,
     "grade_id": "cell-b5926e54adad8548",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, let's import the libraries needed for this assignment.\n",
    "The code used in this notebook is developed based on this github repository: https://github.com/eriklindernoren/PyTorch-YOLOv3/, which is installable as the package `pytorchyolo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -q --upgrade ismi_utils pytorch_lightning pytorchyolo pillow==8.4.0 numpy==1.22 torch==1.11\n",
    "!pip3 install -q --upgrade opencv-python==4.5.5.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d141c309ea45f5079bc89ff07982ab0",
     "grade": false,
     "grade_id": "cell-b44e5fce410d8ca8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorchyolo.models import Darknet\n",
    "from pytorchyolo.detect import detect_image\n",
    "from pytorchyolo.utils.loss import compute_loss\n",
    "from pytorchyolo.utils.utils import rescale_boxes, non_max_suppression\n",
    " \n",
    "# matplotlib is needed to plot bounding boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import xml.etree.ElementTree as ET # needed to read bounding boxes in xml format\n",
    "from pathlib import Path\n",
    "from ismi_utils import download_data\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57dc3f35064d23bad9b38cdfa9a43382",
     "grade": false,
     "grade_id": "cell-0ea692d54cc12cde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Downloading the data\n",
    "Let's get the data and the weights of the pre-trained network needed for this assignment. Use the following cells to download the training, validation and test datasets and set your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8953b1e40394711124bea69ff69cf15b",
     "grade": false,
     "grade_id": "cell-1940473669846069",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "workdir = Path(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0056067b4ffe40a639c1830deeb0843",
     "grade": false,
     "grade_id": "cell-d21a2aa8b90b5d68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Download **training set** and reference standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8897ef2aecdd8be5b533ea66de07e86",
     "grade": false,
     "grade_id": "cell-6ea0d123bfa56918",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = Path(\"train_lung_detection.zip\")\n",
    "download_data(file_name, link='https://surfdrive.surf.nl/files/index.php/s/kwSOdtP4t3dh5vD/download')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fed4d0eb27235e52b9ef66247af80965",
     "grade": false,
     "grade_id": "cell-05610ab9aa287fbd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Download **test set** images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e54410add635411cf7028d0ad3a3285",
     "grade": false,
     "grade_id": "cell-7314081b4f6fd6bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = Path(\"test_images_lung_detection.zip\")\n",
    "download_data(file_name, link='https://surfdrive.surf.nl/files/index.php/s/ZImBQSvSCpqF0i9/download')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ee6b769637532a447ce0e73b934262f",
     "grade": false,
     "grade_id": "cell-b3dba450df30ceba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "At this point, you will see that the structure of ```workdir``` is the following:\n",
    "\n",
    "```python\n",
    "train                        # directory of training data\n",
    "  images                     # training images\n",
    "  xml                        # bounding boxes of right and left lungs \n",
    "  xml_right                  # bounding boxes of only right lung\n",
    "test_images                  # test images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b1c2be235a0dccf28c2e420b5a93a9c",
     "grade": false,
     "grade_id": "cell-a249de21de57e348",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Task 1: Get to know the data and the YOLO network (50 points)\n",
    "\n",
    "In this section, you will:\n",
    "* visualize chest X-ray images from the training set\n",
    "* read bounding box data from the reference standard of the right lung\n",
    "* plot the bounding box\n",
    "\n",
    "After that, you will:\n",
    "* setup a PyTorch Lightning training pipeline\n",
    "* train a YOLO network to detect right lungs\n",
    "* decode the output tensor in order to extract bounding box information\n",
    "* visualize the predicted bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf48e3b89b7363243dfcef481248a226",
     "grade": false,
     "grade_id": "cell-09645dc7e9d73df4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Load chest x-ray from training set\n",
    "To get to know the data you will be working with, first load an example of a chest x-ray image from the training set and visualize it below.\n",
    "\n",
    "**Note:** you can run the cells below multiple times to see the variability in the data! As you will notice, all images have a size of 512x512 px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d439bb9a96230bc32ddab7f94c562237",
     "grade": false,
     "grade_id": "cell-63dcb16cee794a98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define directory of training images\n",
    "train_dir_images = workdir / 'train' / 'images'\n",
    "\n",
    "# Pick random training image\n",
    "case = random.choice(list(train_dir_images.iterdir()))\n",
    "case_filename = Path(case.name).with_suffix('.xml')\n",
    "\n",
    "# Open image with opencv and visualize it\n",
    "with Image.open(case) as im:\n",
    "    image = np.array(im)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2dd062b8b9ff23ba68a864778bb4f43",
     "grade": false,
     "grade_id": "cell-ea689b73243a7e55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question:** Can you explain what is causing the variance in image quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb92f5c617c83c49bdc998514b2cd7",
     "grade": true,
     "grade_id": "cell-eb456398b69cea12",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76ecd00d4b825a8203f520d64ebe57da",
     "grade": false,
     "grade_id": "cell-320474e9b1764b9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "All images in the training data are accompanied by annotations of bounding boxes that surround the right lung. Each annotation consists of a single ```xml``` file with coordinates that define the edges of the boxes (```xmin, xmax, ymin, ymax```).\n",
    "\n",
    "Below you will find code that defines the class ```BoundingBox``` that stores information of bounding boxes, but in a slightly different format, using ```(x, y)``` as the coordinates of the center of the bounding box, and ```(w, h)``` as its width and height. It also implements additional variables related to classes, scores and labels, which will become clear later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e8cdd3dccbd0b9df58314aa8441009a",
     "grade": false,
     "grade_id": "cell-9fbf4f3108f22824",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BoundingBox:\n",
    "    def __init__(self, x, y, w, h, c = None, classes = None):\n",
    "        \"\"\"A bounding box object.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: float\n",
    "            x coordinate of the center of the box\n",
    "        y: float\n",
    "            y coordinate of the center of the box\n",
    "        w: float\n",
    "            width of the box\n",
    "        h: float\n",
    "            height of the box\n",
    "        \"\"\"\n",
    "        self.x     = x\n",
    "        self.y     = y\n",
    "        self.w     = w\n",
    "        self.h     = h\n",
    "        \n",
    "        self.c     = c\n",
    "        self.classes = classes\n",
    "\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)        \n",
    "        return self.label\n",
    "    \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdae03b13c0d8d7dc182682745c24195",
     "grade": false,
     "grade_id": "cell-4151ca95e07eedd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we define a function to read an annotation in ```xml``` format and convert them into parameters to pass to the ```BoundingBox``` class. Most of the code of this function is provided, but you will have to implement the last part of it (replace the ```None``` values).\n",
    "\n",
    "**Note**: as for many variables in the YOLO framework, ```(x, y, w, h)``` used in the ```BoundingBox``` class are normalized by the image height and width, meaning that they have values in the range ```[0.0, 1.0]```. Take this into account in your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2baafee6f9a62919d22a9462b4be9541",
     "grade": true,
     "grade_id": "cell-bdeab055601340c9",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xml2bbox(xml_file):\n",
    "    '''Convert xml file to bounding box\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xml_file: str\n",
    "        Path to xml file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    bbox: BoundingBox object\n",
    "        A BoundingBox object with parameters derived from the xml file\n",
    "    '''\n",
    "    # Parse structure of XML file\n",
    "    tree = ET.parse(xml_file)\n",
    "\n",
    "    for elem in tree.iter():\n",
    "        if 'width' in elem.tag:\n",
    "            width = int(elem.text) # image width\n",
    "        if 'height' in elem.tag:\n",
    "            height = int(elem.text) # image height\n",
    "\n",
    "        for attr in list(elem):\n",
    "\n",
    "            if 'bndbox' in attr.tag:\n",
    "                for dim in list(attr):\n",
    "                    if 'xmin' in dim.tag:\n",
    "                        xmin = int(dim.text) # xmin bounding box\n",
    "                    if 'ymin' in dim.tag:\n",
    "                        ymin = int(dim.text) # ymin bounding box\n",
    "                    if 'xmax' in dim.tag:\n",
    "                        xmax = int(dim.text) # xmax bounding box\n",
    "                    if 'ymax' in dim.tag:\n",
    "                        ymax = int(dim.text) # ymax bounding box\n",
    "    \n",
    "    x = None\n",
    "    y = None\n",
    "    w = None\n",
    "    h = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return BoundingBox(x,y,w,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1f5e9dfe6cf07645a7f6a16bda44173",
     "grade": false,
     "grade_id": "cell-fa8c1a2ecbe7dd9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have defined the ```BoundingBox``` class and the ```xml2bbox``` function, you can get the bounding box corresponding to the XML file annotation for the chosen training image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1ab4f3ceebbe0e9e2b6a7ef8e66e44f",
     "grade": false,
     "grade_id": "cell-442c14e853516dfd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotation_file = workdir / 'train' / 'xml_right' / case_filename\n",
    "bbox = xml2bbox(annotation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f3ef48698bfa82876728e0e9ba9724c",
     "grade": false,
     "grade_id": "cell-bb8a4887b15c1e06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "At this point, it is also convenient to define a function that allows us to plot bounding boxes using ```matplotlib```. We will use the ```Rectangle``` class for that. The following function reads a list of ```BoundingBox``` objects and returns a list of corresponding ```Rectangle``` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb8e59ca58da208e111a1d79e6b82021",
     "grade": false,
     "grade_id": "cell-860cb803458007ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_matplotlib_boxes(boxes, img_shape):\n",
    "    plt_boxes = []\n",
    "    for box in boxes:\n",
    "        xmin  = int((box.x - box.w/2) * img_shape[1])\n",
    "        xmax  = int((box.x + box.w/2) * img_shape[1])\n",
    "        ymin  = int((box.y - box.h/2) * img_shape[0])\n",
    "        ymax  = int((box.y + box.h/2) * img_shape[0])        \n",
    "        plt_boxes.append(patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, fill=False, color='#00FF00', linewidth='2'))\n",
    "    return plt_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af07d6f19976e720046ec1edca672ea9",
     "grade": false,
     "grade_id": "cell-12938755f9b7da67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can use the code written so far to read, process and visualize the bounding box of the right lung together with the chest X-ray image visualized above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73d2167b6007423f31744ba346c1e043",
     "grade": false,
     "grade_id": "cell-cac2451025a06944",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get bounding boxes in matplotlib format\n",
    "plt_boxes = get_matplotlib_boxes([bbox],image.shape)\n",
    "\n",
    "# Visualize image and bounding box\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "for plt_box in plt_boxes:\n",
    "    ax.add_patch(plt_box)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5dedc8eb8cb1b513e0accebb94046a48",
     "grade": false,
     "grade_id": "cell-239b25b63c4fbc0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: Why is it important to indicate the laterality (left/right lung) of the bounding box when working with chest x-rays?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a78a0d850d415e4e5126a4e68f2f5ab",
     "grade": true,
     "grade_id": "cell-bdfab8af205c0155",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Lightning Datamodule\n",
    "\n",
    "In this assignment, you will also get familiar with the [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/) framework. This is a framework on top of PyTorch, which abstracts away a lot of implementation, similar to Keras for Tensorflow. Training models Pytorch Lightning goes best with Lightning Datamodules. This is a single object that contains datasets and dataloaders for training, validating and (optionally) testing models. We start with defining a Dataset. We limit the number of training images here to 100, to limit training time. Later, when training for detection of both lungs, you can extend this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RightLungDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir=None, transform=None):\n",
    "        self.images = self.load_images(image_dir)\n",
    "        self.case_ids = list(self.images.keys())\n",
    "        if label_dir is not None:\n",
    "            self.labels = self.load_labels(label_dir)\n",
    "        else:\n",
    "            self.labels = {case_id: [0, 0, 0, 0, 0] for case_id in self.case_ids} \n",
    "        self.transform = transform\n",
    "        \n",
    "    def load_images(self, data_dir):\n",
    "        images = {}\n",
    "        for image_path in data_dir.iterdir():\n",
    "            case_id = image_path.stem\n",
    "            images[case_id] = image_path\n",
    "            \n",
    "            if len(images) == 100:\n",
    "                break\n",
    "        return images\n",
    "    \n",
    "    def load_image(self, case_id):\n",
    "        with Image.open(self.images[case_id]) as im:\n",
    "            image = np.array(im).astype(np.uint8)\n",
    "        return image\n",
    "    \n",
    "    def load_labels(self, data_dir):\n",
    "        labels = {}\n",
    "        for case_id in self.case_ids:\n",
    "            annotation_file = workdir / 'train' / 'xml_right' / f\"{case_id}.xml\"\n",
    "            bbox = xml2bbox(annotation_file)\n",
    "            labels[case_id] = self.bbox_to_label(bbox)\n",
    "        return labels\n",
    "    \n",
    "    def bbox_to_label(self, bbox):\n",
    "        \"\"\"convert to format [class x_center y_center width height]\"\"\"\n",
    "        return [0, bbox.x, bbox.y, bbox.w, bbox.h]\n",
    "    \n",
    "    def visualize(self, idx):\n",
    "        image, label = self[idx]\n",
    "        \n",
    "        # Remove channel dimension\n",
    "        image = image[0]\n",
    "        label = label[0]\n",
    "        \n",
    "        # Plot x-ray\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image, cmap=\"gray\")\n",
    "        \n",
    "        # Plot box\n",
    "        c, x, y, w, h = label\n",
    "        xmin  = int((x - w/2) * image.shape[1])\n",
    "        xmax  = int((x + w/2) * image.shape[1])\n",
    "        ymin  = int((y - h/2) * image.shape[0])\n",
    "        ymax  = int((y + h/2) * image.shape[0])        \n",
    "        plt_box = patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, fill=False, color='#00FF00', linewidth='2')\n",
    "        ax.add_patch(plt_box)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        case_id = self.case_ids[idx]\n",
    "        image = self.load_image(case_id) / 255  \n",
    "        label = self.labels[case_id]\n",
    "        \n",
    "        # Add channel dimension to image\n",
    "        image = image[None, ...]\n",
    "\n",
    "        # Cast label to correct shape\n",
    "        label = np.array(label).reshape(-1, 5)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your dataset implementation\n",
    "train_dir = workdir / 'train'\n",
    "right_lung_dataset = RightLungDataset(train_dir / \"images\", label_dir = train_dir / \"xml_right\")\n",
    "right_lung_dataset.visualize(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a working Pytorch Dataset, it is relatively straightforward to setup a Lightning Datamodule. Here we have layed out the structure for you. You can refer to the [documentation](https://pytorch-lightning.readthedocs.io/en/stable/data/datamodule.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RightLungDatamodule(pl.LightningDataModule):\n",
    "    def __init__(self, image_dir, label_dir, batch_size=8):\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_fraction = 0.8\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "        \n",
    "    def setup(self, stage):\n",
    "        full_set = RightLungDataset(self.image_dir, self.label_dir)\n",
    "        train_size = int(self.train_fraction*len(full_set))\n",
    "        val_size = len(full_set) - train_size\n",
    "        self.train_dataset, self.val_dataset = random_split(full_set, [train_size, val_size])\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        images, labels = list(zip(*batch))\n",
    "        \n",
    "        # Stack images\n",
    "        images = torch.stack([torch.tensor(image) for image in images])\n",
    "        \n",
    "        # Add sample index to targets\n",
    "        final_labels = []\n",
    "        for i, boxes in enumerate(labels):\n",
    "            boxes = np.concatenate([np.zeros([len(boxes),1]), boxes], axis = 1)\n",
    "            boxes[:, 0] = i\n",
    "            final_labels.append(torch.tensor(boxes))\n",
    "        labels = torch.cat(final_labels, 0)\n",
    "        \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningModule\n",
    "\n",
    "Now that we have setup the data inside the DataModule, it is time to define our model. In Pytorch Lightning, this is done by defining a LightningModule object, which contains your model and code to do a forward pass. The advantage of Lightning is that once you have the LightningModule defined, you can use the Pytorch Lightning Trainer to train your model. The trainer does a lot of heavy lifting for you, so that you need don't need to implement much yourself. Additionally, the Trainer has many useful flags you can set, to modify training behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YOLOv3(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Darknet(\"yolov3-tiny.cfg\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        prediction = self.model(x.float())\n",
    "        return prediction\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, label = batch\n",
    "        prediction = self.forward(image)\n",
    "        loss, _ = compute_loss(prediction, label, self.model)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.train()\n",
    "        image, label = batch\n",
    "        prediction = self.forward(image)\n",
    "        loss, _ = compute_loss(prediction, label, self.model)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the ingredients ready to start training: the DataModule and the LightningModule. The following two cells train the model and plot the loss curves. You can initially run it with the settings we pre-set. If you want, you can experiment later on with changing the learning rate, number of epochs, etc., if you want to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "yolo_v3_model = YOLOv3()\n",
    "\n",
    "# Empty the log directory and setup the logger\n",
    "log_dir = Path(\"logs\") / \"right_lung\" / \"version_0\"\n",
    "if log_dir.is_dir():\n",
    "    shutil.rmtree(log_dir)\n",
    "logger = pl.loggers.CSVLogger(\"logs\", name=\"right_lung\", flush_logs_every_n_steps=1, version=0)\n",
    "\n",
    "# Initialize trainer and train!\n",
    "trainer = pl.Trainer(max_epochs=30, accelerator=\"gpu\", devices=1, logger=logger, log_every_n_steps=1)\n",
    "trainer.fit(yolo_v3_model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot train and validation loss\n",
    "\n",
    "# Read the logged metrics.csv file\n",
    "df = pd.read_csv(Path(\"logs\") / \"right_lung\" / \"version_0\" / \"metrics.csv\")\n",
    "train_loss = df.train_loss.dropna().values\n",
    "val_loss = df.val_loss.dropna().values\n",
    "\n",
    "# Make the plot\n",
    "plt.figure()\n",
    "plt.semilogy(train_loss, label=\"Train loss\")\n",
    "plt.semilogy(val_loss, label=\"Validation loss\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d2a0b011f61f43e28b01a6c90c79bd8",
     "grade": false,
     "grade_id": "cell-79c62fde4370crst",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "When you pass a single image through the network, the raw output of the last yolo layer should have the following shape:\n",
    "\n",
    "```(1, 3, 32, 32, 6)```\n",
    "\n",
    "You can confirm that by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass single image through network\n",
    "image, label = datamodule.train_dataset[0]\n",
    "yolo_v3_model.train()\n",
    "image = torch.tensor(image[None, ...])\n",
    "raw_output = yolo_v3_model(image)[-1]\n",
    "print(f\"Raw output shape last yolo layer: {raw_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "748a2d968f3aea426febe592fbfbac50",
     "grade": false,
     "grade_id": "cell-79c62fde4370c7f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "These values represent:\n",
    "\n",
    "```(mini_batch_size, n_bbox_per_anchor, grid_h, grid_w, ???)```\n",
    "\n",
    "In order to understand the meaning of the 5th dimension (which we indicate as ???), we ask you to do two things:\n",
    "\n",
    "1. Read and try to understand the content of the ```non_max_suppression()``` function used in ```single_inference()``` defined below. You can find the code [here](https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/3467db29ce17c753c519bdcc8a0b1d7bb5601cd8/pytorchyolo/utils/utils.py#L306)\n",
    "2. Use the content of the lecture of this week, about object detection and YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "142b820140083034e370f79bcf4f45b7",
     "grade": false,
     "grade_id": "cell-08ee9fd84085a1ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What is the meaning of the 6 values in the 5th dimension of the raw output of the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b57f57177ba47a296b5093f39a8b9dfc",
     "grade": true,
     "grade_id": "cell-f58439b5e66c8974",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a3dfa344f777d91dc128cf8d87aaa25",
     "grade": false,
     "grade_id": "cell-65acc6425c2cab14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can put all these things together and make a function that we can use to get predicted bounding boxes for a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_inference(model, image, conf_thres=0.02, nms_thres=0.4):\n",
    "    \"\"\"Do inference on a single image\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: LightningModule\n",
    "    image: 3d np.array\n",
    "    conf_thres: float\n",
    "        Confidencen threshold (between 0 and 1) for predicted boxes\n",
    "    nms_thres: float\n",
    "        IoU threshold for non-maximum suppression\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    boxes: list of BoundingBoxes objects\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image = torch.tensor(image[None, ...])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        detections = model(image)\n",
    "        detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "        detections = rescale_boxes(detections[0], 512, (512, 512))\n",
    "    \n",
    "    detections = detections.numpy()\n",
    "    \n",
    "    boxes = []\n",
    "    for detection in detections:\n",
    "        xmin, ymin, xmax, ymax, confidence, class_idx = detection\n",
    "        w = int(xmax - xmin)\n",
    "        h = int(ymax - ymin)\n",
    "        x = int(xmin + w/2)\n",
    "        y = int(ymin + h/2)\n",
    "        boxes.append(BoundingBox(x / 512, y / 512, w / 512, h / 512, c=class_idx))\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e07b6e8fa6f517929d9b274749d25bbe",
     "grade": false,
     "grade_id": "cell-b9a8a00d583c3c6d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We use the function we just defined to process 5 train and 5 validation images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot 5 training images\n",
    "for idx in range(5):\n",
    "    image, label = datamodule.train_dataset[idx]\n",
    "    boxes = single_inference(yolo_v3_model, image)\n",
    "\n",
    "    # Get bounding boxes in matplotlib format\n",
    "    plt_boxes = get_matplotlib_boxes(boxes,image[0].shape)\n",
    "\n",
    "    # Visualize image and bounding box\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    for plt_box in plt_boxes:\n",
    "        ax.add_patch(plt_box)\n",
    "        \n",
    "    plt.title(f\"Training image {idx}\")\n",
    "    \n",
    "# Plot 5 validation images\n",
    "for idx in range(5):\n",
    "    image, label = datamodule.val_dataset[idx]\n",
    "    boxes = single_inference(yolo_v3_model, image)\n",
    "\n",
    "    # Get bounding boxes in matplotlib format\n",
    "    plt_boxes = get_matplotlib_boxes(boxes,image[0].shape)\n",
    "\n",
    "    # Visualize image and bounding box\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    for plt_box in plt_boxes:\n",
    "        ax.add_patch(plt_box)\n",
    "    plt.title(f\"Validation image {idx}\")\n",
    "    \n",
    "gc.collect()  # This prevents RAM from filling up each time we run this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9ae357c5b2ca43feaf7df96a3e4eed9",
     "grade": false,
     "grade_id": "cell-acbf4e9a3af261a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: For this first test, we used a threshold ```conf_thres=0.02```, which (often) results in a meaningful bounding box for the right lung. Try to see what happens if you pick a much lower threshold, for example ```conf_thres=0.0```, or ```conf_thres=0.001``` and explain what you see. Can you find a threshold for which you get one (meaningful) false positive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1741cd9544ae396d56b5e77c18339fa",
     "grade": true,
     "grade_id": "cell-c36be84e5726128a",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10902b728fe0a7c5cc89cd9fb7e2a989",
     "grade": false,
     "grade_id": "cell-757609f2dd392038",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now it would be good to check the performance of YOLO on the entire test set. Since the test set is far too large to process in this manner, if you think that the output of the cell above seems to be correct, run the cell below to process the first 5 images in the test dataset and visualize the output. Again, you can play with the  ```conf_thres``` and observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8f99136c0d76ff071baaf69de193048",
     "grade": false,
     "grade_id": "cell-f354dc241f8753ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir = workdir / 'test_images'\n",
    "test_dataset = RightLungDataset(test_dir)\n",
    "\n",
    "conf_thres = 0.01\n",
    "\n",
    "# Plot 5 test images\n",
    "for idx in range(5):\n",
    "    image, label = test_dataset[idx]\n",
    "    boxes = single_inference(yolo_v3_model, image, conf_thres = conf_thres)\n",
    "\n",
    "    # Get bounding boxes in matplotlib format\n",
    "    plt_boxes = get_matplotlib_boxes(boxes,image[0].shape)\n",
    "\n",
    "    # Visualize image and bounding box\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    for plt_box in plt_boxes:\n",
    "        ax.add_patch(plt_box)\n",
    "    plt.title(f\"Test image {idx}\")\n",
    "    \n",
    "gc.collect()  # This prevents RAM from filling up each time we run this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6ad720402e56ef70567a1167126ebdc",
     "grade": false,
     "grade_id": "cell-2eec0c559eb74c42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Task 2. Re-train YOLO to detect two lungs! (50 points)\n",
    "Now that we have familiarized with the data, the network architecture and the output it produces, we can start re-training YOLO to detect both lungs. First, we look at a bit more detail at the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bb610de6edc8ea55d6b84d4793d4472",
     "grade": false,
     "grade_id": "cell-31d928cbafd5dbfe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loss function\n",
    "We have seen in the lecture that the YOLO network has a loss function that contains several elements. Here we report the formula of the loss function, and if you want you can check out the implementation we are using [here](https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/3467db29ce17c753c519bdcc8a0b1d7bb5601cd8/pytorchyolo/utils/loss.py#L58). As you will see, it's a quite long and complex piece of code. We report the formula and the code here for completeness, but you don't have to worry too much about the details, understanding on a high level is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66360668610619ad31064fdbd8956b07",
     "grade": false,
     "grade_id": "cell-6e18714ff1cebe45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "$$\\begin{multline}\n",
    "\\lambda_\\textbf{coord}\n",
    "\\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "     L_{ij}^{\\text{obj}}\n",
    "            \\left[\n",
    "            \\left(\n",
    "                x_i - \\hat{x}_i\n",
    "            \\right)^2 +\n",
    "            \\left(\n",
    "                y_i - \\hat{y}_i\n",
    "            \\right)^2\n",
    "            \\right]\n",
    "+ \\lambda_\\textbf{coord} \n",
    "\\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "         L_{ij}^{\\text{obj}}\n",
    "         \\left[\n",
    "        \\left(\n",
    "            \\sqrt{w_i} - \\sqrt{\\hat{w}_i}\n",
    "        \\right)^2 +\n",
    "        \\left(\n",
    "            \\sqrt{h_i} - \\sqrt{\\hat{h}_i}\n",
    "        \\right)^2\n",
    "        \\right]\n",
    "+ \\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "        L_{ij}^{\\text{obj}}\n",
    "        \\left(\n",
    "            C_i - \\hat{C}_i\n",
    "        \\right)^2\n",
    "\\\\\n",
    "+ \\lambda_\\textrm{noobj}\n",
    "\\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "    L_{ij}^{\\text{noobj}}\n",
    "        \\left(\n",
    "            C_i - \\hat{C}_i\n",
    "        \\right)^2\n",
    "+ \\sum_{i = 0}^{S^2}\n",
    "L_i^{\\text{obj}}\n",
    "    \\sum_{c \\in \\textrm{classes}}\n",
    "        \\left(\n",
    "            p_i(c) - \\hat{p}_i(c)\n",
    "        \\right)^2\n",
    "\\end{multline}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "979c8a205b0665868eb3b72b289d59a4",
     "grade": false,
     "grade_id": "cell-bd128debb4735f99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: Which of the 5 terms in the loss function penalize the network for predicting bounding boxes in the wrong place? And which term penalizes for the wrong bounding box size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a87340d28ed517d4ecf8769341d5a842",
     "grade": true,
     "grade_id": "cell-534e07c60b26bd3e",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule\n",
    "\n",
    "We will repeat the same steps for the single-lung training, starting with defining a LightningModule based on a Dataset. Here we add an argument ```n_images``` to control the number of training images loaded. In the previous dataset, we already fixed this at 100. You can increase to improve performance, but this will cost you training time. We recommend to experiment with 100 images first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LungsDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir=None, transform=None, n_images=100):\n",
    "        self.n_images = n_images\n",
    "        self.images = self.load_images(image_dir)\n",
    "        self.case_ids = list(self.images.keys())\n",
    "        if label_dir is not None:\n",
    "            self.labels = self.load_labels(label_dir)\n",
    "        else:\n",
    "            self.labels = {case_id: [0, 0, 0, 0, 0] for case_id in self.case_ids} \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def load_images(self, data_dir):\n",
    "        images = {}\n",
    "        for image_path in data_dir.iterdir():\n",
    "            case_id = image_path.stem\n",
    "            images[case_id] = image_path\n",
    "            \n",
    "            if len(images) == self.n_images:\n",
    "                break\n",
    "        return images\n",
    "    \n",
    "    def load_image(self, case_id):\n",
    "        with Image.open(self.images[case_id]) as im:\n",
    "            image = np.array(im).astype(np.uint8)\n",
    "        return image\n",
    "    \n",
    "    def load_labels(self, data_dir):\n",
    "        labels = {}\n",
    "        for case_id in self.case_ids:\n",
    "            annotation_file = data_dir / f\"{case_id}.xml\"\n",
    "            labels[case_id] = self.xml_to_label(annotation_file)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def xml_to_label(self, xml_path):\n",
    "        label = []\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        for box in root.findall(\"object\"):\n",
    "            # Find class index\n",
    "            class_name = box.find(\"name\").text\n",
    "            if class_name == \"RL\":\n",
    "                class_idx = 0\n",
    "            if class_name == \"LL\":\n",
    "                class_idx = 1\n",
    "                \n",
    "            # Find coordinates\n",
    "            coordinates = box.find(\"bndbox\")\n",
    "            xmin = int(coordinates.find(\"xmin\").text)\n",
    "            ymin = int(coordinates.find(\"ymin\").text)\n",
    "            xmax = int(coordinates.find(\"xmax\").text)\n",
    "            ymax = int(coordinates.find(\"ymax\").text)\n",
    "            \n",
    "            x = (xmin + ((xmax - xmin)/2)) / 512\n",
    "            y = (ymin + ((ymax - ymin)/2)) / 512\n",
    "            w = (xmax - xmin) / 512\n",
    "            h = (ymax - ymin) / 512\n",
    "            \n",
    "            label.append([class_idx, x, y, w, h])\n",
    "\n",
    "        return label\n",
    "    \n",
    "    def visualize(self, idx):\n",
    "        image, label = self[idx]\n",
    "        \n",
    "        # Remove channel dimension\n",
    "        image = image[0]\n",
    "\n",
    "        # Plot x-ray\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image, cmap=\"gray\")\n",
    "    \n",
    "        # Plot box\n",
    "        for box_idx in range(label.shape[0]):\n",
    "            box = label[box_idx]\n",
    "            c, x, y, w, h = box\n",
    "            if c == 0:\n",
    "                color = \"green\"\n",
    "                text_label = \"right\"\n",
    "            if c == 1:\n",
    "                color = \"red\"\n",
    "                text_label = \"left\"\n",
    "            \n",
    "            xmin  = int((x - w/2) * image.shape[1])\n",
    "            xmax  = int((x + w/2) * image.shape[1])\n",
    "            ymin  = int((y - h/2) * image.shape[0])\n",
    "            ymax  = int((y + h/2) * image.shape[0])        \n",
    "            plt_box = patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, fill=False, color=color, label = text_label, linewidth='2')\n",
    "            ax.add_patch(plt_box)\n",
    "        plt.legend()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        case_id = self.case_ids[idx]\n",
    "        image = self.load_image(case_id) / 255  \n",
    "        label = self.labels[case_id]\n",
    "        \n",
    "        # Add channel dimension to image\n",
    "        image = image[None, ...]\n",
    "\n",
    "        # Cast label to correct shape\n",
    "        label = np.array(label).reshape(-1, 5)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your dataset implementation\n",
    "train_dir = workdir / 'train'\n",
    "lungs_dataset = LungsDataset(train_dir / \"images\", label_dir = train_dir / \"xml\")\n",
    "lungs_dataset.visualize(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LungsDatamodule(pl.LightningDataModule):\n",
    "    def __init__(self, image_dir, label_dir, n_images = 100, batch_size=8):\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.n_images = n_images\n",
    "        self.train_fraction = 0.8\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "        \n",
    "    def setup(self, stage):\n",
    "        full_set = LungsDataset(self.image_dir, self.label_dir, n_images = self.n_images)\n",
    "        train_size = int(self.train_fraction*len(full_set))\n",
    "        val_size = len(full_set) - train_size\n",
    "        self.train_dataset, self.val_dataset = random_split(full_set, [train_size, val_size])\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        images, labels = list(zip(*batch))\n",
    "        \n",
    "        # Stack images\n",
    "        images = torch.stack([torch.tensor(image) for image in images])\n",
    "        \n",
    "        # Add sample index to targets\n",
    "        final_labels = []\n",
    "        for i, boxes in enumerate(labels):\n",
    "            boxes = np.concatenate([np.zeros([len(boxes),1]), boxes], axis = 1)\n",
    "            boxes[:, 0] = i\n",
    "            final_labels.append(torch.tensor(boxes))\n",
    "        labels = torch.cat(final_labels, 0)\n",
    "        \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a new LightningModule, with a few changes compared to the previous implementation.\n",
    "\n",
    "- There is now an argument in the init that sets the learning rate, for easier experimentation\n",
    "- The model is based on a new config `yolov3-tiny-two-lungs.cfg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YOLOv3_two_lungs(pl.LightningModule):\n",
    "    def __init__(self, lr=1E-3):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.model = Darknet(\"yolov3-tiny-two-lungs.cfg\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        prediction = self.model(x.float())\n",
    "        return prediction\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, label = batch\n",
    "        prediction = self.forward(image)\n",
    "        loss, _ = compute_loss(prediction, label, self.model)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.train()\n",
    "        image, label = batch\n",
    "        prediction = self.forward(image)\n",
    "        loss, _ = compute_loss(prediction, label, self.model)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set train parameters\n",
    "batch_size = 8\n",
    "n_images = 100\n",
    "max_epochs = 10\n",
    "lr = 1E-3\n",
    "\n",
    "# Initialize the model and datamodule\n",
    "yolo_v3_model_two_lungs = YOLOv3_two_lungs(lr = lr)\n",
    "datamodule = LungsDatamodule(train_dir / \"images\", label_dir = train_dir / \"xml\", n_images = n_images, batch_size=batch_size)\n",
    "\n",
    "# Empty the log directory and setup the logger\n",
    "log_dir = Path(\"logs\") / \"two_lungs\" / \"version_0\"\n",
    "if log_dir.is_dir():\n",
    "    shutil.rmtree(log_dir)\n",
    "logger = pl.loggers.CSVLogger(\"logs\", name=\"two_lungs\", flush_logs_every_n_steps=1, version=0)\n",
    "\n",
    "# Initialize trainer and train!\n",
    "trainer = pl.Trainer(max_epochs=max_epochs, accelerator=\"gpu\", devices=1, logger=logger, log_every_n_steps=1)\n",
    "trainer.fit(yolo_v3_model_two_lungs, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot train and validation loss\n",
    "\n",
    "# Read the logged metrics.csv file\n",
    "df = pd.read_csv(Path(\"logs\") / \"two_lungs\" / \"version_0\" / \"metrics.csv\")\n",
    "train_loss = df.train_loss.dropna().values\n",
    "val_loss = df.val_loss.dropna().values\n",
    "\n",
    "# Make the plot\n",
    "plt.figure()\n",
    "plt.semilogy(train_loss, label=\"Train loss\")\n",
    "plt.semilogy(val_loss, label=\"Validation loss\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell implements a new function to visualize predictions (in dashed lines) against the ground truth labels (in solid lines) on train and validation images. The left lung is shown in red, the right lung in green. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_inference_two_lungs(image, label, boxes, title=\"\"):\n",
    "    # Remove channel dimension\n",
    "    image = image[0]\n",
    "\n",
    "    # Plot x-ray\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "\n",
    "    # Plot ground truth boxes\n",
    "    for box_idx in range(label.shape[0]):\n",
    "        box = label[box_idx]\n",
    "        c, x, y, w, h = box\n",
    "        if c == 0:\n",
    "            color = \"green\"\n",
    "            text_label = \"right\"\n",
    "        if c == 1:\n",
    "            color = \"red\"\n",
    "            text_label = \"left\"\n",
    "\n",
    "        xmin  = int((x - w/2) * image.shape[1])\n",
    "        xmax  = int((x + w/2) * image.shape[1])\n",
    "        ymin  = int((y - h/2) * image.shape[0])\n",
    "        ymax  = int((y + h/2) * image.shape[0])        \n",
    "        plt_box = patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, fill=False, color=color, label = text_label, linewidth='2')\n",
    "        ax.add_patch(plt_box)\n",
    "        \n",
    "    # Plot predicted boxes\n",
    "    for box in boxes:\n",
    "        c, x, y, w, h = box.c, box.x, box.y, box.w, box.h\n",
    "        if c == 0:\n",
    "            color = \"green\"\n",
    "            text_label = \"right\"\n",
    "        if c == 1:\n",
    "            color = \"red\"\n",
    "            text_label = \"left\"\n",
    "\n",
    "        xmin  = int((x - w/2) * image.shape[1])\n",
    "        xmax  = int((x + w/2) * image.shape[1])\n",
    "        ymin  = int((y - h/2) * image.shape[0])\n",
    "        ymax  = int((y + h/2) * image.shape[0])        \n",
    "        plt_box = patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, fill=False, color=color, label = text_label, linewidth='2', linestyle=\"--\")\n",
    "        ax.add_patch(plt_box)\n",
    "   \n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following cell to tune the confidence and non-maximum suppression thresholds visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf_thres=0.5\n",
    "nms_thres=0.5\n",
    "\n",
    "# Plot 5 training images\n",
    "for idx in range(5):\n",
    "    image, label = datamodule.train_dataset[idx]\n",
    "    boxes = single_inference(yolo_v3_model_two_lungs, image, conf_thres=conf_thres, nms_thres = nms_thres)\n",
    "    visualize_inference_two_lungs(image, label, boxes, title=f\"Training image {idx}\")\n",
    "    \n",
    "# Plot 5 validation images\n",
    "for idx in range(5):\n",
    "    image, label = datamodule.val_dataset[idx]\n",
    "    boxes = single_inference(yolo_v3_model_two_lungs, image, conf_thres=conf_thres, nms_thres = nms_thres)\n",
    "    visualize_inference_two_lungs(image, label, boxes, title=f\"Validation image {idx}\")\n",
    "    \n",
    "gc.collect()  # This prevents RAM from filling up each time we run this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ba97f596fe262c6772e60b8d521d440",
     "grade": false,
     "grade_id": "cell-01a074be0d3c43e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "At this point, experiment with the various training and inference parameters until you are satisfied with the validation performance. The final step is to upload your results to grand challenge, to see how you stack up to your fellow students. Running the next cell gives you a csv that you can upload here: https://ismi-chestxray.grand-challenge.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference parameters\n",
    "conf_thres = 0.5\n",
    "nms_thres = 0.5\n",
    "\n",
    "test_dir = workdir / 'test_images'\n",
    "header = 'filename,RL_xmin,RL_ymin,RL_xmax,RL_ymax,LL_xmin,LL_ymin,LL_xmax,LL_ymax\\n'\n",
    "csv_file = 'test_set.csv'\n",
    "\n",
    "# Load checkpoint\n",
    "for ckpt_path in (Path(\"logs\") / \"two_lungs\" / \"version_0\" / \"checkpoints\").glob(\"*.ckpt\"):\n",
    "    print(ckpt_path)\n",
    "    model = YOLOv3_two_lungs.load_from_checkpoint(ckpt_path)\n",
    "model.eval()\n",
    "\n",
    "with open(csv_file, 'w') as csv_out:\n",
    "    csv_out.write(header)\n",
    "\n",
    "for image_path in tqdm(list(test_dir.iterdir())):\n",
    "    with Image.open(image_path) as im:\n",
    "        image = np.array(im).astype(np.uint8)\n",
    "    image = image / 255  \n",
    "    image = image[None, ...]\n",
    "    image = torch.tensor(image[None, ...])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        detections = model(image)\n",
    "        detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "        detections = rescale_boxes(detections[0], 512, (512, 512))\n",
    "    \n",
    "    detections = detections.numpy()\n",
    "\n",
    "    # Pick highest confidence right lung\n",
    "    right_lung = [0, 0, 0, 0]  # By default we find no box\n",
    "    for box_idx in range(detections.shape[0]):\n",
    "        xmin, ymin, xmax, ymax, confidence, class_idx = detections[box_idx]\n",
    "        if class_idx == 0:\n",
    "            right_lung = [int(xmin), int(ymin), int(xmax), int(ymax)]\n",
    "            break\n",
    "\n",
    "    # Pick highest confidence left lung\n",
    "    left_lung = [0, 0, 0, 0]  # By default we find no box\n",
    "    for box_idx in range(detections.shape[0]):\n",
    "        xmin, ymin, xmax, ymax, confidence, class_idx = detections[box_idx]\n",
    "        if class_idx == 1:\n",
    "            left_lung = [int(xmin), int(ymin), int(xmax), int(ymax)]\n",
    "            break\n",
    "    \n",
    "    line = f\"{image_path.stem},{right_lung[0]},{right_lung[1]},{right_lung[2]},{right_lung[3]},{left_lung[0]},{left_lung[1]},{left_lung[2]},{left_lung[3]}\"\n",
    "\n",
    "    with open(csv_file, 'a') as csv_out:\n",
    "        csv_out.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final questions task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afb18c0820a4e7c45b66a3079a007c8f",
     "grade": false,
     "grade_id": "cell-6be18c5fa9deac32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: We have not explicitly evaluated the performance of the networks in this assignment. Suppose we want to evaluate the left lung detection performance with a *case-based AUC score*, this means giving each case a single score and calculating the AUC over the entire dataset. Describe how you would calculate the case-based ROC/AUC from a set of bounding boxes with confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a60c6aedecd72d2392ae580df0cdc0d",
     "grade": true,
     "grade_id": "cell-0307b2a846f671dd",
     "locked": false,
     "points": 12,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d01a1e3b1033e4986266861ac20bc6be",
     "grade": false,
     "grade_id": "cell-b08a5fad2b64f80a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: Is the case-based AUC score a good choice of metric for evaluating left lung detection performance? Explain why/why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5034c1dfe3c551cfbc42dd3a425fba8",
     "grade": true,
     "grade_id": "cell-f0b2461b8f5fd47b",
     "locked": false,
     "points": 12,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38dddbecc301d5bc44a2d826a616dba4",
     "grade": false,
     "grade_id": "cell-a441a64af3894e55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: Suppose that instead of the big dataset of this assignment (>10,000 images), you only had access to 100 chest X-ray images. You have to ask a radiologist to annotate your data and can ask him to either (1) annotate the lungs with bounding boxes, or (2) annotate with a lung segmentation. Which choice would probably give you the best performing model and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef242c769f0e8b1e6efbcb6c03f2cb4e",
     "grade": true,
     "grade_id": "cell-fc4f56fe91b532f9",
     "locked": false,
     "points": 16,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2582d727ef60763894851ef3d61a5da",
     "grade": false,
     "grade_id": "cell-7e20b948d9d3a91c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Task 3: Improve your results! (0 points)\n",
    "\n",
    "The main message of the steps of this assignment so far is that YOLO is a powerful architecture for object detection, and that it works very well for lungs detection in chest x-ray images. There are, however, still quite a few things unexplored and probably you can improve your performance by a lot. If you are up for a challenge, use the empty cells below to explore one or both of the following:\n",
    "\n",
    "- Configure the architecture by copying `yolov3-tiny-two-lungs.cfg` and editing parameters.\n",
    "- Explore data augmentation, by passing a `transform` to the dataset/dataloader.\n",
    "\n",
    "For grading and feedback purposes, it helps if you document your exploration, either with figures or comments/text in cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
