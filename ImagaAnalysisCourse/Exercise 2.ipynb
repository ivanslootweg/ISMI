{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd3033e",
   "metadata": {},
   "source": [
    "# Image Analysis 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187637d6",
   "metadata": {},
   "source": [
    "This Jupyter notebook is part of the course Image Analysis from Radboud University (Nijmegen, Netherlands), and it was developed by researchers of Radboud University Medical Center (Nijmegen, Netherlands).\n",
    "\n",
    "You should have obtained this notebook by downloading it from the official Brightspace page of the course.\n",
    "If this is not the case, you should contact the course coordinator at this email address: geert.litjens@radboudumc.nl\n",
    "\n",
    "This notebook formulates an assignment as part of the course, and the content of this notebook should be used solely to develop a solution to this assignment.\n",
    "You should not make the code provided in this notebook, or your own solution, publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc7b2e",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` by substituting `None` variables or by adding your own solution and any place that says \"YOUR ANSWER HERE\" with your answers to the questions. Note that it is perfectly fine to substitute the images in the exercises with your own if you want to. Please fill in your name and collaborators below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7367f1",
   "metadata": {},
   "source": [
    "## Students\n",
    "Please fill in this cell with your name and e-mail address. This information will be used to track completion of the assignments.\n",
    "\n",
    "* Name student #1, email address: ...\n",
    "* Name student #2, email address (optional): ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f766c1",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "* Groups: You should work in **groups of maximum 2 people**.\n",
    "* Deadline for this assignment: \n",
    " * Preferably before April 28th\n",
    " * Send your **fully executed** notebook to: geert.litjens@radboudumc.nl\n",
    " * Or upload to BrightSpace\n",
    "* The file name of the notebook you submit must be ```NameSurname1_NameSurname2.ipynb```\n",
    "\n",
    "This notebooks contains cells with snippets of code that we provide in order to load and visualize data, but also some convenience functions that could be useful to develop your assignment.\n",
    "\n",
    "We also provide templates for functions that have to be implemented, with a given list of input variables and some output variables.\n",
    "\n",
    "Your submission should contain the **fully executed** notebook with **your code** implemented, as well as **your answers** to questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8dfdf7",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f2a5e",
   "metadata": {},
   "source": [
    "First, we import the basic libraries necessary to develop this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33183941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage as ski # For reading images\n",
    "import skimage.transform as skit # Basic image transformation functions\n",
    "import skimage.segmentation as ssegm\n",
    "import skimage.draw as sdraw\n",
    "from   skimage.filters import threshold_otsu\n",
    "from   skimage.color import label2rgb\n",
    "import skimage.morphology as smorp\n",
    "import matplotlib.pyplot as plt # Plotting and visalization of images\n",
    "import numpy as np # Basic math and array functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f50cb8",
   "metadata": {},
   "source": [
    "# Thresholding\n",
    "We'll start with a grayscale image. Let's see how well regular and automated thresholing techniques compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5c2e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page = ski.data.page() # Get page image as Numpy array\n",
    "plt.imshow(page, cmap='gray'); # Show the image\n",
    "print(\"Image shape: \" + str(page.shape)) # Print the shape (i.e. dimenions) of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d73432",
   "metadata": {},
   "source": [
    "As a first exercise, we'll try to separte the text from the background. A sensible first attempt would be to try to manually define a threshold that might work. We can just do trial and error, but looking at an image histogram is generally a sensible strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4748a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the histogram uses one range more (257 vs. 256) because the last bin has a closed end (e.g. with\n",
    "# 256 the last bin will cover both 254 and 255, which we do not want)\n",
    "hist, bins = np.histogram(page, range(257))\n",
    "plt.bar(range(256), hist);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b2e27",
   "metadata": {},
   "source": [
    "Not a very straightforward histogram to select a threshold for. The background is white, so that's probably the peak at the end. Let's try a threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23945915",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc779cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4)) \n",
    "axes[0].imshow(page, cmap=\"gray\"); # Show the orignal image\n",
    "axes[1].imshow(page > t, cmap=\"gray\"); # Show the thresholded image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b358eb",
   "metadata": {},
   "source": [
    "That is not a great result. We can manually optimize it, but we can also use an automated thresholding methods. Let's try to implement Otsu's method from the lecture. It operates under the assumption that we want to discriminate two classes, which we want to do here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f8913",
   "metadata": {},
   "source": [
    "Remember from the lecture that Otsu's threshold aims to minimize within-class variance, which is given as:\n",
    "$$\\sigma _{w}^{2}(t)=\\omega _{0}(t)\\sigma _{0}^{2}(t)+\\omega _{1}(t)\\sigma _{1}^{2}(t)$$\n",
    "this is the same as maximizing the between class variance (i.e. making the distance in value between the average pixel in class 1 and class 2 as high as possible). This can be written as:\n",
    "$$\\sigma _{b}^{2}(t)=\\omega _{0}(t)\\omega _{1}(t)[\\mu _{0}(t) - \\mu _{1}(t)]^{2}$$\n",
    "Here $\\omega _{0}$ and $\\omega _{1}$ are the probabilities that a pixel belongs to class 0 and class 1, which can be calculated by counting the number of pixels in each class (which depends on the threshold) and then subdividing by the total number of pixels in the image. Furthermore, $\\mu _{0}$ and $\\mu _{1}$ are the average pixel values for each class. To find the optimal threshold, you will need to calculate these four values for every possible threshold, calculate $\\sigma _{b}^{2}$, and pick the threshold for which this value is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the None's in the function below\n",
    "def otsu_threshold(hist, bins):\n",
    "    inter_class_variances = []\n",
    "    total_pixels = np.sum(hist)\n",
    "    for t in range(1, 255): # Iterate over all possible threshold values (excluding the first and final values)\n",
    "        \n",
    "        # Get the weights\n",
    "        weight1 = None\n",
    "        weight2 = None\n",
    "\n",
    "        # Get the class means mu0(t)\n",
    "        mean1 = None\n",
    "        # Get the class means mu1(t)\n",
    "        mean2 = None\n",
    "\n",
    "        inter_class_variances.append(weight1 * weight2 * (mean1 - mean2) ** 2)\n",
    "\n",
    "    # Maximize the inter_class_variance function val\n",
    "    index_of_max_val = np.argmax(inter_class_variances)\n",
    "\n",
    "    threshold = bins[:-1][index_of_max_val]\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a0a3b",
   "metadata": {},
   "source": [
    "We can compare our implementation against the implementation in scikit-image to see if it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_otsu = otsu_threshold(hist, bins)\n",
    "t_otsu_skimage = threshold_otsu(page, nbins=256)\n",
    "print(\"Our implementation: \" + str(t_otsu) + \", official implementation: \" + str(t_otsu_skimage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4)) # This creates a plot with 3 horizontal subplots\n",
    "axes[0].imshow(page, cmap=\"gray\");\n",
    "axes[1].imshow(page > t, cmap=\"gray\");\n",
    "axes[2].imshow(page > t_otsu, cmap=\"gray\");\n",
    "axes[3].imshow(page > t_otsu_skimage, cmap=\"gray\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646c956",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question:** Even with the automated threshold calculation, we do not get a perfect segmentation result due to the background illumination. Can you come up with a strategy that improves on this result, but while using the exact same methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a146192",
   "metadata": {},
   "source": [
    "# Connected Component Analysis and Region Growing\n",
    "Now let's move to instance segmentation, where we try to identify every single entity of a class in an image separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848aea17",
   "metadata": {},
   "source": [
    "### Connected component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42044dcf",
   "metadata": {},
   "source": [
    "First, let's load an image which has a lot of different instances of the same class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coins = ski.data.coins()\n",
    "plt.imshow(coins, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d063f1f",
   "metadata": {},
   "source": [
    "We will use a slightly more fancy method to segment the individual samples, specifically Sobel filtering (which we will discuss in the next lecture) and a Watershed Transformation (https://www.wikiwand.com/en/Watershed_(image_processing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e744d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = ski.filters.sobel(coins)\n",
    "markers = np.zeros_like(coins)\n",
    "foreground, background = 1, 2\n",
    "markers[coins < 30.0] = background\n",
    "markers[coins > 150.0] = foreground\n",
    "\n",
    "segmented_coins = ssegm.watershed(edges, markers) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1785c4",
   "metadata": {},
   "source": [
    "This results in a nice semantic segmentation, but all the instances have the same label, as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax[0].imshow(coins, cmap=plt.cm.gray)\n",
    "ax[0].set_title(\"Original\")\n",
    "\n",
    "label_image = label2rgb(segmented_coins, coins, bg_label=0, bg_color=None, kind=\"overlay\")\n",
    "ax[1].imshow(label_image)\n",
    "ax[1].set_title(\"Segmented coins\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56667043",
   "metadata": {},
   "source": [
    "Below I have implemented the two functions needed to perform connected component analysis. The first handles the connectivity of the pixel under consideration and currently assumes 4-connectivity. The latter uses this function to perform the connected component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7134c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nbs(label_image, row, col, connected_labels):\n",
    "    # We start with the assumption that the previous label is 0\n",
    "    prev_label = 0\n",
    "    \n",
    "    # For 4-connectivity, we need to check two potential previous labels, left and up from the current position,\n",
    "    # in 8-connectivity we also need to check the diagonal.\n",
    "    up_label = label_image[row - 1, col]\n",
    "    left_label = label_image[row, col - 1]\n",
    "    \n",
    "    # First we check whether any of the previous labels is 0, because then we can stop because there is no previous\n",
    "    # label in the neighborhood\n",
    "    if up_label > 0 or left_label > 0:\n",
    "        # Subsequently, we need to check whether one previous label is higher than the other, because we want to keep\n",
    "        # track of connected labels (to later replace them) and because we want to return the lowest of the two.\n",
    "        if up_label > left_label:\n",
    "            # Here we check whether the lower valued label is actually higher than 0, otherwise there is only\n",
    "            # one previous label and we can simply return that. Otherwise we need to register the connectivity\n",
    "            # between the different labels and return the lowest label.\n",
    "            if left_label > 0:\n",
    "                connected_labels[up_label].add(left_label)\n",
    "                return left_label\n",
    "            else:\n",
    "                return up_label\n",
    "        # Repetion of the statement above\n",
    "        elif left_label > up_label:\n",
    "            if up_label > 0:\n",
    "                connected_labels[left_label].add(up_label)\n",
    "                return up_label\n",
    "            else:\n",
    "                return left_label\n",
    "        else:\n",
    "            return up_label\n",
    "    else:\n",
    "        return prev_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91fca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connected_components(image):\n",
    "    label_image = np.zeros_like(image, dtype=\"uint32\")\n",
    "    connected_labels = {}\n",
    "    cur_label = 0\n",
    "    # First pass, iterate over all pixels\n",
    "    for row in range(image.shape[0]):\n",
    "        for col in range(image.shape[1]):\n",
    "            # If we encounter an object (value = 1), check whether one of the earlier visited neighbors also was\n",
    "            # an object and which label it got. The neighbors that are checked depend on the connectivity (4 or 8 in 2D)\n",
    "            if image[row, col] == 1:\n",
    "                prev_label = check_nbs(label_image, row, col, connected_labels)\n",
    "                if prev_label == 0: # New object found\n",
    "                    cur_label += 1\n",
    "                    label_image[row, col] = cur_label\n",
    "                    connected_labels[cur_label] = set()\n",
    "                else:\n",
    "                    label_image[row,col] = prev_label\n",
    "    \n",
    "    # Here we remap every label to the lowest label it is connected to\n",
    "    mapping = [0]\n",
    "    for k in sorted(connected_labels.keys()):\n",
    "        if connected_labels[k]:\n",
    "            cur_lab = k\n",
    "            while connected_labels[cur_lab]:\n",
    "                cur_lab = min(connected_labels[cur_lab])\n",
    "            mapping.append(cur_lab)\n",
    "        else:\n",
    "            mapping.append(k)\n",
    "        \n",
    "    # Second pass relabels all labels based on the new mapping\n",
    "    for row in range(image.shape[0]):\n",
    "        for col in range(image.shape[1]):\n",
    "            label_image[row, col] = mapping[label_image[row, col]]\n",
    "    return label_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa6707",
   "metadata": {},
   "source": [
    "Below shows the result of the connected component analysis: all instances are nicely separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e832a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax[0].imshow(coins, cmap=plt.cm.gray)\n",
    "ax[0].set_title(\"Original\")\n",
    "\n",
    "labeled_coins = get_connected_components(segmented_coins)\n",
    "label_image = label2rgb(labeled_coins, coins, bg_label=0, bg_color=None, kind=\"overlay\")\n",
    "ax[1].imshow(label_image)\n",
    "ax[1].set_title(\"Segmented coins\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f3c85",
   "metadata": {},
   "source": [
    "**Assignment**: Below you have a test image for which we can also identify the connected components. However, 4-connectedness is not enoug here. Can you modify the `check_nbs` function above to handle 8-connectivity? You do not need to modify the other function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b20986",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.zeros((220, 220), dtype=\"uint8\")\n",
    "test_image[10:50, 10:50] = 1\n",
    "test_image[50:100, 50:100] = 1\n",
    "test_image[10:50, 100:140] = 1\n",
    "test_image[150:200, 150:200] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_connected_components(test_image));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1607f57f",
   "metadata": {},
   "source": [
    "### Region growing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6276c268",
   "metadata": {},
   "source": [
    "We performed the instance segmentation with a relatively complicated set of methods. We can also use an easier method, based on region growing, that requires only a single algorithm. However, it does require some manual work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d12a6a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coins = ski.data.coins()\n",
    "plt.imshow(coins, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3918f",
   "metadata": {},
   "source": [
    "Below we segment the first coin using the `flood` function. We need to specify a tolerance which determines how similar the gray value pf the adjacent pixels has to be to the seed point to be included in the segmentation. You can visualize this as well below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_point = (50, 50)\n",
    "tolerance = 30\n",
    "coin_mask = ssegm.flood(coins, seed_point, connectivity=2, tolerance=tolerance).astype(\"ubyte\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50813de0",
   "metadata": {},
   "source": [
    "**Assignment:** This cell extracts the rest of the coins from the first row. Modify seed_points and tolerances to segment all coins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_points = [None]\n",
    "tolerances = [None]\n",
    "for coin_nr in range(5):\n",
    "    coin_mask += (coin_nr + 2) * ssegm.flood(coins, seed_points[coin_nr], connectivity=2, tolerance=tolerances[coin_nr]).astype(\"ubyte\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501f8c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax[0].imshow(coins, cmap=plt.cm.gray)\n",
    "ax[0].scatter(seed_point[1], seed_point[0])\n",
    "ax[0].set_title(\"Original\")\n",
    "\n",
    "label_image = label2rgb(coin_mask, coins, bg_label=0, bg_color=None, kind=\"overlay\")\n",
    "ax[1].imshow(label_image)\n",
    "ax[1].scatter(seed_point[1], seed_point[0])\n",
    "ax[1].set_title(\"Segmented coin\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002ee5f",
   "metadata": {},
   "source": [
    "# Mathematical Morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe8ebc",
   "metadata": {},
   "source": [
    "The last topic of this week is mathematical morphology. We will experiment with different use-cases, both using standard morphological operations, reconstruction, and greyscale morphology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db72a4",
   "metadata": {},
   "source": [
    "### Erosion and dilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b245a4b9",
   "metadata": {},
   "source": [
    "Below is the first binary image we will experiment with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "horse = ski.data.horse().astype(\"ubyte\")\n",
    "plt.imshow(horse, cmap=\"gray\"); # Show the image\n",
    "print(\"Image shape: \" + str(horse.shape)) # Print the shape (i.e. dimenions) of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aecf9c2",
   "metadata": {},
   "source": [
    "**Assignment:** Extract the border from the horse using a morphological operation. You can either use `smorp.erosion` or `smorp.dilation`. You need to do one extra step in addition to the morphological operation. Remember you can use `?smorp.erosion` to get an explanation of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64034a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_border = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16a52a",
   "metadata": {},
   "source": [
    "Show the results below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d006285",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 8))\n",
    "\n",
    "ax[0].imshow(horse, cmap=plt.cm.gray)\n",
    "ax[0].set_title(\"Original\")\n",
    "ax[1].imshow(horse_border, cmap=plt.cm.gray)\n",
    "ax[1].set_title(\"Border\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bec9f2",
   "metadata": {},
   "source": [
    "### Hole filling using morphological reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb534f9",
   "metadata": {},
   "source": [
    "The function below creates and artifical image with holes. Let's try to get them filled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "holes_image = np.zeros((256, 256), dtype=\"ubyte\")\n",
    "holes_image[50:100, 50:100] = 1\n",
    "holes_image[55:65, 55:65] = 0\n",
    "holes_image[85:90, 75:80] = 0\n",
    "holes_image[175:225, 175:225] = 1\n",
    "holes_image[200:215, 200:215] = 0\n",
    "holes_image[105:115, 50:100] = 1\n",
    "circ_pos = sdraw.disk((75, 200), 30)\n",
    "circ_hole_pos = sdraw.disk((75, 200), 10)\n",
    "holes_image[circ_pos] = 1\n",
    "holes_image[circ_hole_pos] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c539ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(holes_image, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a73768",
   "metadata": {},
   "source": [
    "First, let's try to use a standard *closing* operation, which is a dilation followed by an erosion. We will use a 20-pixel disk as a structuring element. The plot function shows tbe dilated and subsequently the eroded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilated_image = smorp.dilation(holes_image, selem=smorp.disk(20))\n",
    "eroded_image =  smorp.erosion(dilated_image, selem=smorp.disk(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28678bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax[0].imshow(dilated_image, cmap=plt.cm.gray)\n",
    "ax[0].set_title(\"Original\")\n",
    "ax[1].imshow(eroded_image, cmap=plt.cm.gray)\n",
    "ax[1].set_title(\"Contrast Strecthced\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a25df74",
   "metadata": {},
   "source": [
    "As you can see, the holes are filled, but we have an artificat: the bar below the top-left square is now fused with the square, which is not what we want. Let's see if we can do better with morphological reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b4495",
   "metadata": {},
   "source": [
    "Remember that for morphological reconstruction we work with two images: a marker image on which the morphological operations are applied and the mask image, which is used to intersect and limit the marker image. Both are created in the cell below. The marker image is an image filled with 0's, except for the borders which are filled with 1's and are the starting points for our morphological operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae781ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_image = np.ones_like(holes_image)\n",
    "marker_image[1:-1,1:-1] = 0\n",
    "mask_image = 1 - holes_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba8605",
   "metadata": {},
   "source": [
    "**Assignment:** Adapt the function below to perform the reconstruction. You will need to replace the `None` statements. The first needs to apply the morphological operation, the second needs to use the mask image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_image = marker_image.copy()\n",
    "last_image = np.zeros_like(marker_image)\n",
    "while (recon_image != last_image).any():\n",
    "    last_image = recon_image.copy()\n",
    "    dilated_image = None\n",
    "    recon_image = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367196b",
   "metadata": {},
   "source": [
    "The function below should show the test image with the holes filled, but without any of the artifacts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5c62d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(1 - recon_image, cmap = \"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c268600",
   "metadata": {},
   "source": [
    "### Greyscale Morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3330a0",
   "metadata": {},
   "source": [
    "The last topic is greyscale morphological reconstruction. Specifically, we are going to use it to correct illumincation differences and improve our thresholding operations. You do not need to implement anything, but you have to explain what is happening in your own words. First we make the marker and mask images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a6895",
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_image = np.copy(ski.data.page())\n",
    "marker_image[1:-1, 1:-1] = ski.data.page().max()\n",
    "mask_image = ski.data.page()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a5b03",
   "metadata": {},
   "source": [
    "Now perform the reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_image = marker_image.copy()\n",
    "last_image = np.zeros_like(marker_image)\n",
    "while (recon_image != last_image).any():\n",
    "    last_image = recon_image.copy()\n",
    "    eroded_image = smorp.erosion(recon_image, selem=np.ones((3,3)))\n",
    "    recon_image = np.maximum(eroded_image, mask_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f12e6",
   "metadata": {},
   "source": [
    "Calculate the illumination corrected image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5821499",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = recon_image - ski.data.page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67beb88c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(corrected, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d35745",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_otsu_improved = threshold_otsu(corrected, nbins=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386227fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4)) # This creates a plot with 3 horizontal subplots\n",
    "axes[0].imshow(page, cmap=\"gray\"); \n",
    "axes[1].imshow(page > t_otsu, cmap=\"gray\");\n",
    "axes[2].imshow(corrected < t_otsu_improved, cmap=\"gray\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad94e5",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question:** Explain, based on the definition of greyscale morphological operations, what the above process is doing in each step. For example, what happens when we perform the greyscale erosion on the marker image? You can plot the intermediate images if you want to see it visually."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
